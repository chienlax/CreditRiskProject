{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# --- Sklearn ---\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (roc_auc_score, roc_curve, precision_recall_curve, auc,\n",
    "                             accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, brier_score_loss, log_loss, classification_report)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# --- TabNet & PyTorch ---\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Utility functions + Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions (can be imported from a utils file or redefined) ---\n",
    "def calculate_ks(y_true, y_prob):\n",
    "    \"\"\"Calculates the Kolmogorov-Smirnov (KS) statistic.\"\"\"\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_prob': y_prob})\n",
    "    df = df.sort_values(by='y_prob', ascending=False)\n",
    "    # Ensure y_true sums are not zero before division\n",
    "    sum_true = df['y_true'].sum()\n",
    "    sum_false = len(df) - sum_true\n",
    "    if sum_true == 0 or sum_false == 0:\n",
    "        return 0.0 # KS is 0 if one class is missing\n",
    "    df['cumulative_true'] = df['y_true'].cumsum() / sum_true\n",
    "    df['cumulative_false'] = (1 - df['y_true']).cumsum() / sum_false\n",
    "    ks = max(abs(df['cumulative_true'] - df['cumulative_false']))\n",
    "    return ks\n",
    "\n",
    "def find_optimal_threshold_j_statistic(y_true, y_prob_oof):\n",
    "    \"\"\"Finds the optimal threshold maximizing Youden's J statistic (Sensitivity + Specificity - 1).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob_oof)\n",
    "     # Handle cases where thresholds might not be strictly decreasing\n",
    "    valid_indices = np.where(np.isfinite(thresholds))[0]\n",
    "    if len(valid_indices) == 0:\n",
    "        print(\"Warning: No valid thresholds found for J-statistic calculation.\")\n",
    "        return 0.5 # Default fallback\n",
    "    fpr, tpr, thresholds = fpr[valid_indices], tpr[valid_indices], thresholds[valid_indices]\n",
    "\n",
    "    if len(thresholds) == 0:\n",
    "         print(\"Warning: Threshold array is empty after filtering.\")\n",
    "         return 0.5\n",
    "\n",
    "    j_statistic = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_statistic)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    # Ensure threshold is within [0, 1] bounds if necessary due to floating point issues\n",
    "    optimal_threshold = max(0.0, min(1.0, optimal_threshold))\n",
    "    print(f\"Optimal threshold based on Youden's J-Statistic (OOF): {optimal_threshold:.4f}\")\n",
    "    return optimal_threshold\n",
    "\n",
    "def evaluate_model(y_true, y_pred_proba, y_pred_binary, model_name=\"Model\"):\n",
    "    \"\"\"Calculates and prints standard classification metrics.\"\"\"\n",
    "    # Add epsilon to probabilities for log_loss if necessary\n",
    "    eps = 1e-15\n",
    "    y_pred_proba = np.clip(y_pred_proba, eps, 1 - eps)\n",
    "\n",
    "    auc_roc = roc_auc_score(y_true, y_pred_proba)\n",
    "    gini = 2 * auc_roc - 1\n",
    "    ks = calculate_ks(y_true, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "    precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "    brier = brier_score_loss(y_true, y_pred_proba)\n",
    "    logloss = log_loss(y_true, y_pred_proba)\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "\n",
    "    print(f\"\\n--- Evaluation Metrics for {model_name} ---\")\n",
    "    print(f\"AUC ROC:        {auc_roc:.4f}\")\n",
    "    print(f\"Gini Coefficient: {gini:.4f}\")\n",
    "    print(f\"KS Statistic:   {ks:.4f}\")\n",
    "    print(f\"Accuracy:       {accuracy:.4f}\")\n",
    "    print(f\"Precision:      {precision:.4f}\")\n",
    "    print(f\"Recall (TPR):   {recall:.4f}\")\n",
    "    print(f\"F1-Score:       {f1:.4f}\")\n",
    "    print(f\"Brier Score:    {brier:.4f}\")\n",
    "    print(f\"Log Loss:       {logloss:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'AUC': auc_roc, 'Gini': gini, 'KS': ks, 'Accuracy': accuracy,\n",
    "        'Precision': precision, 'Recall': recall, 'F1': f1,\n",
    "        'Brier': brier, 'LogLoss': logloss\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def plot_roc_curve(y_true, y_prob, model_name):\n",
    "    \"\"\"Plots the ROC curve.\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    auc_roc = roc_auc_score(y_true, y_prob)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_roc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # Diagonal line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} - ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # Save the plot\n",
    "    plot_filename = f\"roc_curve_{model_name.replace(' ', '_')}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"ROC curve saved to {plot_filename}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "DATA_PATH = '../data/processed/'\n",
    "MODEL_OUTPUT_PATH = './tabnet_outputs/' # Directory to save model/results\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "SEED = 86\n",
    "N_SPLITS = 10\n",
    "TARGET = 'TARGET'\n",
    "ID_COL = 'SK_ID_CURR'\n",
    "\n",
    "# --- TabNet Refined High-Complexity Parameter Set ---\n",
    "TABNET_PARAMS = dict(\n",
    "    # --- Network Architecture (Higher Capacity) ---\n",
    "    n_d=64,             # Increased\n",
    "    n_a=64,             # Increased\n",
    "    n_steps=5,            # Increased\n",
    "    n_independent=2,     # Increased\n",
    "    n_shared=2,          # Increased\n",
    "    mask_type='sparsemax', # Keep\n",
    "\n",
    "    # --- Regularization (Balanced Approach) ---\n",
    "    gamma=1.5,           # Moderate reuse encouragement (Increased from 1.1)\n",
    "    lambda_sparse=1e-4,    # Moderate sparsity penalty (Increased from 1e-6)\n",
    "\n",
    "    # --- Optimizer Settings ---\n",
    "    optimizer_fn=torch.optim.AdamW,\n",
    "    optimizer_params=dict(lr=1e-3), # Slightly higher initial LR might explore faster\n",
    "\n",
    "    # --- Learning Rate Scheduler ---\n",
    "    scheduler_fn=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "    scheduler_params=dict(\n",
    "        T_0=30,          # Number of epochs for the first restart cycle (TUNABLE)\n",
    "        T_mult=1,          # Factor to increase T_i after a restart (1 = cycles stay same length)\n",
    "        eta_min=1e-6,      # Minimum learning rate boundary\n",
    "    ),\n",
    "\n",
    "    # --- Other ---\n",
    "    seed=SEED,\n",
    "    verbose=1,\n",
    "    clip_value=1.0         # Keep gradient clipping\n",
    ")\n",
    "\n",
    "# --- Training Configuration (Adjust for Higher Complexity) ---\n",
    "NUM_CYCLES = 30\n",
    "CYCLE_LENGTH = TABNET_PARAMS['scheduler_params']['T_0']\n",
    "MAX_EPOCHS = NUM_CYCLES * CYCLE_LENGTH  # e.g., 15 * 25 = 375 epochs\n",
    "PATIENCE = MAX_EPOCHS + 1\n",
    "BATCH_SIZE = 16384 # Start here, maybe even 512 if OOM\n",
    "VIRTUAL_BATCH_SIZE = 4096 # Adjust proportionally (e.g., 128 if BATCH_SIZE=512)\n",
    "\n",
    "# --- Check for GPU ---\n",
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data ---\n",
    "print(\"Loading preprocessed data...\")\n",
    "try:\n",
    "    train_df = pd.read_csv(DATA_PATH + 'train_final.csv')\n",
    "    test_df = pd.read_csv(DATA_PATH + 'test_final.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Make sure 'train_final.csv' and 'test_final.csv' are in {DATA_PATH}\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Data ---\n",
    "y_train = train_df[TARGET].values # Use .values for numpy arrays\n",
    "y_test = test_df[TARGET].values\n",
    "\n",
    "# Drop Target and potentially ID\n",
    "if ID_COL in train_df.columns:\n",
    "    X_train = train_df.drop(columns=[TARGET, ID_COL])\n",
    "    X_test = test_df.drop(columns=[TARGET, ID_COL])\n",
    "else:\n",
    "    X_train = train_df.drop(columns=[TARGET])\n",
    "    X_test = test_df.drop(columns=[TARGET])\n",
    "\n",
    "# Align columns just in case\n",
    "common_cols = list(X_train.columns.intersection(X_test.columns))\n",
    "X_train = X_train[common_cols]\n",
    "X_test = X_test[common_cols]\n",
    "feature_names = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying Low Variance Feature Selection...\n",
      "Original number of features: 773\n",
      "Number of features after variance thresholding: 542\n",
      "Updated X_train shape: (246005, 542)\n",
      "Updated X_test shape: (61502, 542)\n"
     ]
    }
   ],
   "source": [
    "# --- Feature Selection: Low Variance Filter ---\n",
    "print(\"\\nApplying Low Variance Feature Selection...\")\n",
    "var_selector = VarianceThreshold(threshold=0.01)\n",
    "\n",
    "# Fit on training data only\n",
    "var_selector.fit(X_train)\n",
    "\n",
    "# Get the boolean mask of selected features\n",
    "feature_mask = var_selector.get_support()\n",
    "original_feature_names = X_train.columns.tolist() # Get original names before transformation\n",
    "selected_feature_names = [name for name, selected in zip(original_feature_names, feature_mask) if selected]\n",
    "\n",
    "print(f\"Original number of features: {X_train.shape[1]}\")\n",
    "print(f\"Number of features after variance thresholding: {len(selected_feature_names)}\")\n",
    "\n",
    "# Transform both X_train and X_test\n",
    "X_train_np_selected = var_selector.transform(X_train)\n",
    "X_test_np_selected = var_selector.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame with selected column names\n",
    "X_train = pd.DataFrame(X_train_np_selected, columns=selected_feature_names, index=X_train.index)\n",
    "X_test = pd.DataFrame(X_test_np_selected, columns=selected_feature_names, index=X_test.index)\n",
    "\n",
    "# Update the global feature_names list\n",
    "feature_names = selected_feature_names\n",
    "\n",
    "print(f\"Updated X_train shape: {X_train.shape}\")\n",
    "print(f\"Updated X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Treating all features as numerical for TabNet due to pre-encoded input data.\n",
      "Prepared X_train shape: (246005, 542)\n",
      "Prepared X_test shape: (61502, 542)\n"
     ]
    }
   ],
   "source": [
    "# --- LIMITATION: Treat all features as numerical ---\n",
    "# Ideally, identify original categorical features and pass their indices to TabNet.\n",
    "# Since we are using pre-encoded data, we treat all as numerical.\n",
    "print(\"WARNING: Treating all features as numerical for TabNet due to pre-encoded input data.\")\n",
    "categorical_indices = [] # No categorical indices provided\n",
    "categorical_dims = [] # No specific dimensions needed if indices are empty\n",
    "\n",
    "# Handle infinite values and NaNs\n",
    "# Replace inf with NaN, then fill NaN with median of each column\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(X_train.median())\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(X_train.median()) # Use train median for test set\n",
    "\n",
    "# Standardize the data\n",
    "# Note: TabNet can handle raw data, but standardization may help in some cases.\n",
    "# You can comment this out if you prefer to use raw data.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to numpy arrays of type float32 for PyTorch\n",
    "X_train_np = X_train.astype(np.float32)\n",
    "X_test_np = X_test.astype(np.float32)\n",
    "\n",
    "print(f\"Prepared X_train shape: {X_train_np.shape}\")\n",
    "print(f\"Prepared X_test shape: {X_test_np.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class weights: [ 1.       11.386959] to emphasize minority class\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights inversely proportional to class frequencies\n",
    "class_counts = np.bincount(y_train)\n",
    "total_samples = len(y_train)\n",
    "# More aggressive weighting for minority class\n",
    "class_weights = torch.tensor([1.0, (class_counts[0]/class_counts[1]) * 1.0], dtype=torch.float32)\n",
    "if device_name == 'cuda':\n",
    "    class_weights = class_weights.cuda()\n",
    "\n",
    "print(f\"Using class weights: {class_weights.cpu().numpy()} to emphasize minority class\")\n",
    "\n",
    "# Create weighted loss function\n",
    "weighted_loss = torch.nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Baseline TabNet highest Model without CV...\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "CUDA device name: NVIDIA GeForce RTX 3060\n",
      "Current CUDA device: 0\n",
      "Using device: cuda for TabNet model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet highest model initialized on cuda\n",
      "Fitting baseline TabNet highest model...\n",
      "epoch 0  | loss: 1.52226 | validation_auc: 0.51301 | validation_balanced_accuracy: 0.50605 |  0:00:03s\n",
      "epoch 1  | loss: 1.21598 | validation_auc: 0.51527 | validation_balanced_accuracy: 0.50182 |  0:00:06s\n",
      "epoch 2  | loss: 1.14579 | validation_auc: 0.51825 | validation_balanced_accuracy: 0.50549 |  0:00:09s\n",
      "epoch 3  | loss: 1.03835 | validation_auc: 0.52892 | validation_balanced_accuracy: 0.51794 |  0:00:13s\n",
      "epoch 4  | loss: 0.98599 | validation_auc: 0.52747 | validation_balanced_accuracy: 0.51916 |  0:00:16s\n",
      "epoch 5  | loss: 0.96579 | validation_auc: 0.52944 | validation_balanced_accuracy: 0.52315 |  0:00:19s\n",
      "epoch 6  | loss: 0.93228 | validation_auc: 0.53432 | validation_balanced_accuracy: 0.52335 |  0:00:22s\n",
      "epoch 7  | loss: 0.91754 | validation_auc: 0.53752 | validation_balanced_accuracy: 0.53168 |  0:00:25s\n",
      "epoch 8  | loss: 0.8838  | validation_auc: 0.53999 | validation_balanced_accuracy: 0.52884 |  0:00:29s\n",
      "epoch 9  | loss: 0.86849 | validation_auc: 0.54063 | validation_balanced_accuracy: 0.52858 |  0:00:32s\n",
      "epoch 10 | loss: 0.85349 | validation_auc: 0.54388 | validation_balanced_accuracy: 0.53182 |  0:00:35s\n",
      "epoch 11 | loss: 0.83807 | validation_auc: 0.55282 | validation_balanced_accuracy: 0.53909 |  0:00:38s\n",
      "epoch 12 | loss: 0.82388 | validation_auc: 0.558   | validation_balanced_accuracy: 0.54192 |  0:00:41s\n",
      "epoch 13 | loss: 0.81359 | validation_auc: 0.56665 | validation_balanced_accuracy: 0.5484  |  0:00:45s\n",
      "epoch 14 | loss: 0.80725 | validation_auc: 0.5722  | validation_balanced_accuracy: 0.54982 |  0:00:48s\n",
      "epoch 15 | loss: 0.80007 | validation_auc: 0.57697 | validation_balanced_accuracy: 0.55558 |  0:00:51s\n",
      "epoch 16 | loss: 0.79206 | validation_auc: 0.5786  | validation_balanced_accuracy: 0.5542  |  0:00:54s\n",
      "epoch 17 | loss: 0.78693 | validation_auc: 0.58486 | validation_balanced_accuracy: 0.56025 |  0:00:57s\n",
      "epoch 18 | loss: 0.7846  | validation_auc: 0.59072 | validation_balanced_accuracy: 0.56192 |  0:01:00s\n",
      "epoch 19 | loss: 0.77565 | validation_auc: 0.59631 | validation_balanced_accuracy: 0.5697  |  0:01:04s\n",
      "epoch 20 | loss: 0.776   | validation_auc: 0.59766 | validation_balanced_accuracy: 0.57292 |  0:01:07s\n",
      "epoch 21 | loss: 0.76296 | validation_auc: 0.59524 | validation_balanced_accuracy: 0.56664 |  0:01:10s\n",
      "epoch 22 | loss: 0.76878 | validation_auc: 0.596   | validation_balanced_accuracy: 0.56708 |  0:01:13s\n",
      "epoch 23 | loss: 0.76376 | validation_auc: 0.60054 | validation_balanced_accuracy: 0.57678 |  0:01:16s\n",
      "epoch 24 | loss: 0.76114 | validation_auc: 0.60318 | validation_balanced_accuracy: 0.57446 |  0:01:19s\n",
      "epoch 25 | loss: 0.75676 | validation_auc: 0.60402 | validation_balanced_accuracy: 0.57475 |  0:01:23s\n",
      "epoch 26 | loss: 0.75978 | validation_auc: 0.59976 | validation_balanced_accuracy: 0.57157 |  0:01:26s\n",
      "epoch 27 | loss: 0.76438 | validation_auc: 0.60456 | validation_balanced_accuracy: 0.57458 |  0:01:29s\n",
      "epoch 28 | loss: 0.76051 | validation_auc: 0.60414 | validation_balanced_accuracy: 0.57462 |  0:01:32s\n",
      "epoch 29 | loss: 0.76195 | validation_auc: 0.59841 | validation_balanced_accuracy: 0.57131 |  0:01:36s\n",
      "epoch 30 | loss: 0.75952 | validation_auc: 0.6065  | validation_balanced_accuracy: 0.57791 |  0:01:39s\n",
      "epoch 31 | loss: 0.75776 | validation_auc: 0.61486 | validation_balanced_accuracy: 0.58094 |  0:01:42s\n",
      "epoch 32 | loss: 0.75084 | validation_auc: 0.62417 | validation_balanced_accuracy: 0.58568 |  0:01:45s\n",
      "epoch 33 | loss: 0.74018 | validation_auc: 0.62141 | validation_balanced_accuracy: 0.58885 |  0:01:49s\n",
      "epoch 34 | loss: 0.72841 | validation_auc: 0.63215 | validation_balanced_accuracy: 0.59403 |  0:01:52s\n",
      "epoch 35 | loss: 0.72231 | validation_auc: 0.6331  | validation_balanced_accuracy: 0.59169 |  0:01:55s\n",
      "epoch 36 | loss: 0.71751 | validation_auc: 0.6448  | validation_balanced_accuracy: 0.60438 |  0:01:58s\n",
      "epoch 37 | loss: 0.71204 | validation_auc: 0.63925 | validation_balanced_accuracy: 0.59945 |  0:02:01s\n",
      "epoch 38 | loss: 0.7029  | validation_auc: 0.65158 | validation_balanced_accuracy: 0.60934 |  0:02:04s\n",
      "epoch 39 | loss: 0.70093 | validation_auc: 0.65247 | validation_balanced_accuracy: 0.60983 |  0:02:07s\n",
      "epoch 40 | loss: 0.69479 | validation_auc: 0.65881 | validation_balanced_accuracy: 0.6137  |  0:02:10s\n",
      "epoch 41 | loss: 0.69119 | validation_auc: 0.65726 | validation_balanced_accuracy: 0.61192 |  0:02:13s\n",
      "epoch 42 | loss: 0.6899  | validation_auc: 0.66606 | validation_balanced_accuracy: 0.61803 |  0:02:16s\n",
      "epoch 43 | loss: 0.68309 | validation_auc: 0.66416 | validation_balanced_accuracy: 0.62062 |  0:02:20s\n",
      "epoch 44 | loss: 0.68238 | validation_auc: 0.66579 | validation_balanced_accuracy: 0.62332 |  0:02:23s\n",
      "epoch 45 | loss: 0.67782 | validation_auc: 0.66933 | validation_balanced_accuracy: 0.62348 |  0:02:26s\n",
      "epoch 46 | loss: 0.67666 | validation_auc: 0.67086 | validation_balanced_accuracy: 0.62435 |  0:02:29s\n",
      "epoch 47 | loss: 0.67499 | validation_auc: 0.67172 | validation_balanced_accuracy: 0.62476 |  0:02:33s\n",
      "epoch 48 | loss: 0.66941 | validation_auc: 0.6765  | validation_balanced_accuracy: 0.62797 |  0:02:36s\n",
      "epoch 49 | loss: 0.66792 | validation_auc: 0.6748  | validation_balanced_accuracy: 0.62952 |  0:02:39s\n",
      "epoch 50 | loss: 0.66727 | validation_auc: 0.67747 | validation_balanced_accuracy: 0.63104 |  0:02:42s\n",
      "epoch 51 | loss: 0.66954 | validation_auc: 0.67717 | validation_balanced_accuracy: 0.63081 |  0:02:46s\n",
      "epoch 52 | loss: 0.66525 | validation_auc: 0.67602 | validation_balanced_accuracy: 0.62766 |  0:02:49s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFitting baseline TabNet highest model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mtabnet_baseline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_base\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalidation\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbalanced_accuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPATIENCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVIRTUAL_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweighted_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m end_time = time.time()\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBaseline model training completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:258\u001b[39m, in \u001b[36mTabModel.fit\u001b[39m\u001b[34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_epochs):\n\u001b[32m    254\u001b[39m \n\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28mself\u001b[39m._callback_container.on_epoch_begin(epoch_idx)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m     \u001b[38;5;66;03m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, valid_dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:489\u001b[39m, in \u001b[36mTabModel._train_epoch\u001b[39m\u001b[34m(self, train_loader)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[32m    487\u001b[39m     \u001b[38;5;28mself\u001b[39m._callback_container.on_batch_begin(batch_idx)\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     batch_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28mself\u001b[39m._callback_container.on_batch_end(batch_idx, batch_logs)\n\u001b[32m    493\u001b[39m epoch_logs = {\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._optimizer.param_groups[-\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:527\u001b[39m, in \u001b[36mTabModel._train_batch\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.network.parameters():\n\u001b[32m    525\u001b[39m     param.grad = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m output, M_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.compute_loss(output, y)\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Add the overall sparsity loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:616\u001b[39m, in \u001b[36mTabNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    615\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.embedder(x)\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtabnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:492\u001b[39m, in \u001b[36mTabNetNoEmbeddings.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    491\u001b[39m     res = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     steps_output, M_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     res = torch.sum(torch.stack(steps_output, dim=\u001b[32m0\u001b[39m), dim=\u001b[32m0\u001b[39m)\n\u001b[32m    495\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_multi_task:\n\u001b[32m    496\u001b[39m         \u001b[38;5;66;03m# Result will be in list format\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:181\u001b[39m, in \u001b[36mTabNetEncoder.forward\u001b[39m\u001b[34m(self, x, prior)\u001b[39m\n\u001b[32m    179\u001b[39m M_feature_level = torch.matmul(M, \u001b[38;5;28mself\u001b[39m.group_attention_matrix)\n\u001b[32m    180\u001b[39m masked_x = torch.mul(M_feature_level, x)\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeat_transformers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m d = ReLU()(out[:, : \u001b[38;5;28mself\u001b[39m.n_d])\n\u001b[32m    183\u001b[39m steps_output.append(d)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:737\u001b[39m, in \u001b[36mFeatTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshared\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    738\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.specifics(x)\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:772\u001b[39m, in \u001b[36mGLU_Block.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m772\u001b[39m     scale = torch.sqrt(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.first:  \u001b[38;5;66;03m# the first layer of the block has no scale multiplication\u001b[39;00m\n\u001b[32m    774\u001b[39m         x = \u001b[38;5;28mself\u001b[39m.glu_layers[\u001b[32m0\u001b[39m](x)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Initialize and Train a Baseline TabNet Model ---\n",
    "print(\"\\nTraining Baseline TabNet highest Model without CV...\")\n",
    "\n",
    "# First, explicitly check and print GPU information\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    # Force CUDA device if available\n",
    "    device_name = 'cuda'\n",
    "else:\n",
    "    print(\"Warning: CUDA not available, using CPU\")\n",
    "    device_name = 'cpu'\n",
    "\n",
    "# Make sure TABNET_PARAMS has device_name set correctly\n",
    "TABNET_PARAMS['device_name'] = device_name\n",
    "print(f\"Using device: {device_name} for TabNet model\")\n",
    "\n",
    "# Split off a small portion of the training data for validation (early stopping purposes)\n",
    "X_train_base, X_val_base, y_train_base, y_val_base = train_test_split(\n",
    "    X_train_np, y_train, test_size=0.20, random_state=SEED, stratify=y_train\n",
    ")\n",
    "\n",
    "# Initialize the TabNet model with the parameters defined in TABNET_PARAMS\n",
    "tabnet_baseline = TabNetClassifier(**TABNET_PARAMS)\n",
    "print(f\"TabNet highest model initialized on {device_name}\")\n",
    "\n",
    "# Train the model with early stopping based on validation set\n",
    "print(\"Fitting baseline TabNet highest model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "tabnet_baseline.fit(\n",
    "    X_train=X_train_base, y_train=y_train_base,\n",
    "    eval_set=[(X_val_base, y_val_base)],\n",
    "    eval_name=['validation'],\n",
    "    eval_metric=['auc', 'balanced_accuracy'],\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    virtual_batch_size=VIRTUAL_BATCH_SIZE,\n",
    "    drop_last=False,\n",
    "    loss_fn=weighted_loss,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Baseline model training completed in {(end_time - start_time):.2f} seconds.\")\n",
    "\n",
    "# Make predictions\n",
    "baseline_preds = tabnet_baseline.predict_proba(X_test_np)[:, 1]\n",
    "baseline_binary_preds = (baseline_preds > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the baseline model\n",
    "print(\"\\n--- Baseline TabNet Model Evaluation ---\")\n",
    "baseline_results = evaluate_model(\n",
    "    y_test, baseline_preds, baseline_binary_preds, model_name=\"TabNet_highest (Baseline)\"\n",
    ")\n",
    "\n",
    "# Plot ROC curve\n",
    "plot_roc_curve(y_test, baseline_preds, \"TabNet_highest (Baseline)\")\n",
    "\n",
    "# Save the model\n",
    "baseline_model_path = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_highest_baseline_model\")\n",
    "saved_baseline_path = tabnet_baseline.save_model(baseline_model_path)\n",
    "print(f\"Baseline model saved to {saved_baseline_path}\")\n",
    "\n",
    "# Save baseline results\n",
    "baseline_results_df = pd.DataFrame([baseline_results])\n",
    "baseline_results_filename = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_highest_baseline_results.csv\")\n",
    "baseline_results_df.to_csv(baseline_results_filename, index=False, mode='w+')\n",
    "print(f\"Baseline results saved to {baseline_results_filename}\")\n",
    "\n",
    "# Feature importance from baseline model\n",
    "baseline_importance = tabnet_baseline.feature_importances_\n",
    "baseline_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': baseline_importance})\n",
    "baseline_importance_df = baseline_importance_df.sort_values(by='Importance', ascending=False)\n",
    "baseline_importance_filename = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_highest_baseline_feature_importance.csv\")\n",
    "baseline_importance_df.to_csv(baseline_importance_filename, index=False, mode='w+')\n",
    "print(f\"Baseline feature importance saved to {baseline_importance_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold based on Youden's J-Statistic (OOF): 0.5004\n",
      "Optimal threshold for validation set: 0.5004\n",
      "\n",
      "--- Baseline TabNet Model Evaluation with Optimal Threshold ---\n",
      "\n",
      "--- Evaluation Metrics for TabNet_Highest (Baseline with Optimal Threshold) ---\n",
      "AUC ROC:        0.6477\n",
      "Gini Coefficient: 0.2955\n",
      "KS Statistic:   0.2154\n",
      "Accuracy:       0.6008\n",
      "Precision:      0.1186\n",
      "Recall (TPR):   0.6133\n",
      "F1-Score:       0.1988\n",
      "Brier Score:    0.2325\n",
      "Log Loss:       0.6570\n",
      "\n",
      "Confusion Matrix:\n",
      "[[33907 22630]\n",
      " [ 1920  3045]]\n",
      "Baseline results with optimal threshold saved to ./tabnet_outputs/tabnet_highest_baseline_results_optimal.csv\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal threshold using the validation set\n",
    "optimal_threshold = find_optimal_threshold_j_statistic(y_val_base, tabnet_baseline.predict_proba(X_val_base)[:, 1])\n",
    "print(f\"Optimal threshold for validation set: {optimal_threshold:.4f}\")\n",
    "\n",
    "# Apply the optimal threshold to the test set predictions\n",
    "baseline_binary_preds_optimal = (baseline_preds > optimal_threshold).astype(int)\n",
    "# Evaluate the model with the optimal threshold\n",
    "print(\"\\n--- Baseline TabNet Model Evaluation with Optimal Threshold ---\")\n",
    "baseline_results_optimal = evaluate_model(\n",
    "    y_test, baseline_preds, baseline_binary_preds_optimal, model_name=\"TabNet_Highest (Baseline with Optimal Threshold)\"\n",
    ")\n",
    "\n",
    "# Save the results with the optimal threshold\n",
    "baseline_results_optimal_df = pd.DataFrame([baseline_results_optimal])\n",
    "baseline_results_optimal_filename = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_highest_baseline_results_optimal.csv\")\n",
    "baseline_results_optimal_df.to_csv(baseline_results_optimal_filename, index=False, mode='w+')\n",
    "print(f\"Baseline results with optimal threshold saved to {baseline_results_optimal_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
