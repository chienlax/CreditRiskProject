{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# --- Sklearn ---\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (roc_auc_score, roc_curve, precision_recall_curve, auc,\n",
    "                             accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, brier_score_loss, log_loss, classification_report)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# --- TabNet & PyTorch ---\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Utility functions + Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions (can be imported from a utils file or redefined) ---\n",
    "def calculate_ks(y_true, y_prob):\n",
    "    \"\"\"Calculates the Kolmogorov-Smirnov (KS) statistic.\"\"\"\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_prob': y_prob})\n",
    "    df = df.sort_values(by='y_prob', ascending=False)\n",
    "    # Ensure y_true sums are not zero before division\n",
    "    sum_true = df['y_true'].sum()\n",
    "    sum_false = len(df) - sum_true\n",
    "    if sum_true == 0 or sum_false == 0:\n",
    "        return 0.0 # KS is 0 if one class is missing\n",
    "    df['cumulative_true'] = df['y_true'].cumsum() / sum_true\n",
    "    df['cumulative_false'] = (1 - df['y_true']).cumsum() / sum_false\n",
    "    ks = max(abs(df['cumulative_true'] - df['cumulative_false']))\n",
    "    return ks\n",
    "\n",
    "def find_optimal_threshold_j_statistic(y_true, y_prob_oof):\n",
    "    \"\"\"Finds the optimal threshold maximizing Youden's J statistic (Sensitivity + Specificity - 1).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob_oof)\n",
    "     # Handle cases where thresholds might not be strictly decreasing\n",
    "    valid_indices = np.where(np.isfinite(thresholds))[0]\n",
    "    if len(valid_indices) == 0:\n",
    "        print(\"Warning: No valid thresholds found for J-statistic calculation.\")\n",
    "        return 0.5 # Default fallback\n",
    "    fpr, tpr, thresholds = fpr[valid_indices], tpr[valid_indices], thresholds[valid_indices]\n",
    "\n",
    "    if len(thresholds) == 0:\n",
    "         print(\"Warning: Threshold array is empty after filtering.\")\n",
    "         return 0.5\n",
    "\n",
    "    j_statistic = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_statistic)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    # Ensure threshold is within [0, 1] bounds if necessary due to floating point issues\n",
    "    optimal_threshold = max(0.0, min(1.0, optimal_threshold))\n",
    "    print(f\"Optimal threshold based on Youden's J-Statistic (OOF): {optimal_threshold:.4f}\")\n",
    "    return optimal_threshold\n",
    "\n",
    "def evaluate_model(y_true, y_pred_proba, y_pred_binary, model_name=\"Model\"):\n",
    "    \"\"\"Calculates and prints standard classification metrics.\"\"\"\n",
    "    # Add epsilon to probabilities for log_loss if necessary\n",
    "    eps = 1e-15\n",
    "    y_pred_proba = np.clip(y_pred_proba, eps, 1 - eps)\n",
    "\n",
    "    auc_roc = roc_auc_score(y_true, y_pred_proba)\n",
    "    gini = 2 * auc_roc - 1\n",
    "    ks = calculate_ks(y_true, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "    precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "    brier = brier_score_loss(y_true, y_pred_proba)\n",
    "    logloss = log_loss(y_true, y_pred_proba)\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "\n",
    "    print(f\"\\n--- Evaluation Metrics for {model_name} ---\")\n",
    "    print(f\"AUC ROC:        {auc_roc:.4f}\")\n",
    "    print(f\"Gini Coefficient: {gini:.4f}\")\n",
    "    print(f\"KS Statistic:   {ks:.4f}\")\n",
    "    print(f\"Accuracy:       {accuracy:.4f}\")\n",
    "    print(f\"Precision:      {precision:.4f}\")\n",
    "    print(f\"Recall (TPR):   {recall:.4f}\")\n",
    "    print(f\"F1-Score:       {f1:.4f}\")\n",
    "    print(f\"Brier Score:    {brier:.4f}\")\n",
    "    print(f\"Log Loss:       {logloss:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'AUC': auc_roc, 'Gini': gini, 'KS': ks, 'Accuracy': accuracy,\n",
    "        'Precision': precision, 'Recall': recall, 'F1': f1,\n",
    "        'Brier': brier, 'LogLoss': logloss\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def plot_roc_curve(y_true, y_prob, model_name):\n",
    "    \"\"\"Plots the ROC curve.\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    auc_roc = roc_auc_score(y_true, y_prob)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_roc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # Diagonal line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} - ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # Save the plot\n",
    "    plot_filename = f\"roc_curve_{model_name.replace(' ', '_')}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"ROC curve saved to {plot_filename}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "DATA_PATH = '../data/processed/'\n",
    "MODEL_OUTPUT_PATH = './tabnet_outputs/' # Directory to save model/results\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "SEED = 42\n",
    "N_SPLITS = 10\n",
    "TARGET = 'TARGET'\n",
    "ID_COL = 'SK_ID_CURR'\n",
    "\n",
    "# --- Check for GPU ---\n",
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data ---\n",
    "print(\"Loading preprocessed data...\")\n",
    "try:\n",
    "    train_df = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "    test_df = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Make sure 'train.csv' and 'test.csv' are in {DATA_PATH}\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Data ---\n",
    "y_train = train_df[TARGET].values # Use .values for numpy arrays\n",
    "y_test = test_df[TARGET].values\n",
    "\n",
    "# Drop Target and potentially ID\n",
    "if ID_COL in train_df.columns:\n",
    "    X_train = train_df.drop(columns=[TARGET, ID_COL])\n",
    "    X_test = test_df.drop(columns=[TARGET, ID_COL])\n",
    "else:\n",
    "    X_train = train_df.drop(columns=[TARGET])\n",
    "    X_test = test_df.drop(columns=[TARGET])\n",
    "\n",
    "# Align columns just in case\n",
    "common_cols = list(X_train.columns.intersection(X_test.columns))\n",
    "X_train = X_train[common_cols]\n",
    "X_test = X_test[common_cols]\n",
    "feature_names = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Treating all features as numerical for TabNet due to pre-encoded input data.\n",
      "Prepared X_train shape: (246005, 112)\n",
      "Prepared X_test shape: (61502, 112)\n"
     ]
    }
   ],
   "source": [
    "# --- LIMITATION: Treat all features as numerical ---\n",
    "# Ideally, identify original categorical features and pass their indices to TabNet.\n",
    "# Since we are using pre-encoded data, we treat all as numerical.\n",
    "print(\"WARNING: Treating all features as numerical for TabNet due to pre-encoded input data.\")\n",
    "categorical_indices = [] # No categorical indices provided\n",
    "categorical_dims = [] # No specific dimensions needed if indices are empty\n",
    "\n",
    "# Handle infinite values and NaNs\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(X_train.median())\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(X_train.median()) \n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_np = X_train.values.astype(np.float32)\n",
    "X_test_np = X_test.values.astype(np.float32)\n",
    "\n",
    "print(f\"Prepared X_train shape: {X_train_np.shape}\")\n",
    "print(f\"Prepared X_test shape: {X_test_np.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class weights: [ 1.       11.386959] to emphasize minority class\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights inversely proportional to class frequencies\n",
    "class_counts = np.bincount(y_train)\n",
    "total_samples = len(y_train)\n",
    "class_weights = torch.tensor([1.0, (class_counts[0]/class_counts[1]) *1.0], dtype=torch.float32)\n",
    "if device_name == 'cuda':\n",
    "    class_weights = class_weights.cuda()\n",
    "\n",
    "print(f\"Using class weights: {class_weights.cpu().numpy()} to emphasize minority class\")\n",
    "\n",
    "weighted_loss = torch.nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Baseline TabNet 10 Model without CV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet 10 model initialized on cuda\n",
      "Fitting baseline TabNet 10 model...\n",
      "epoch 0  | loss: 1.68918 | validation_logloss: 3.15587 | validation_accuracy: 0.73027 | validation_auc: 0.49991 | validation_balanced_accuracy: 0.50215 |  0:01:11s\n",
      "epoch 1  | loss: 1.58672 | validation_logloss: 2.71221 | validation_accuracy: 0.7247  | validation_auc: 0.50744 | validation_balanced_accuracy: 0.49683 |  0:02:21s\n",
      "epoch 2  | loss: 1.47381 | validation_logloss: 3.41773 | validation_accuracy: 0.58881 | validation_auc: 0.49868 | validation_balanced_accuracy: 0.50088 |  0:03:28s\n",
      "epoch 3  | loss: 1.37522 | validation_logloss: 3.34591 | validation_accuracy: 0.49562 | validation_auc: 0.50197 | validation_balanced_accuracy: 0.4998  |  0:04:35s\n",
      "epoch 4  | loss: 1.38865 | validation_logloss: 3.44503 | validation_accuracy: 0.39698 | validation_auc: 0.49964 | validation_balanced_accuracy: 0.50023 |  0:05:41s\n",
      "epoch 5  | loss: 1.34723 | validation_logloss: 2.28376 | validation_accuracy: 0.45586 | validation_auc: 0.51249 | validation_balanced_accuracy: 0.5109  |  0:06:40s\n",
      "epoch 6  | loss: 1.3288  | validation_logloss: 1.61506 | validation_accuracy: 0.4957  | validation_auc: 0.5131  | validation_balanced_accuracy: 0.5142  |  0:07:40s\n",
      "epoch 7  | loss: 1.32096 | validation_logloss: 1.33351 | validation_accuracy: 0.46849 | validation_auc: 0.51289 | validation_balanced_accuracy: 0.51317 |  0:08:41s\n",
      "epoch 8  | loss: 1.32494 | validation_logloss: 1.57498 | validation_accuracy: 0.31721 | validation_auc: 0.50179 | validation_balanced_accuracy: 0.5115  |  0:09:41s\n",
      "epoch 9  | loss: 1.32612 | validation_logloss: 1.33796 | validation_accuracy: 0.32944 | validation_auc: 0.49792 | validation_balanced_accuracy: 0.50679 |  0:10:41s\n",
      "epoch 10 | loss: 1.30563 | validation_logloss: 1.37433 | validation_accuracy: 0.31558 | validation_auc: 0.50546 | validation_balanced_accuracy: 0.51016 |  0:12:01s\n",
      "epoch 11 | loss: 1.50293 | validation_logloss: 1.87151 | validation_accuracy: 0.20077 | validation_auc: 0.49509 | validation_balanced_accuracy: 0.50776 |  0:12:35s\n",
      "epoch 12 | loss: 1.55816 | validation_logloss: 1.42672 | validation_accuracy: 0.27321 | validation_auc: 0.4915  | validation_balanced_accuracy: 0.49951 |  0:13:09s\n",
      "epoch 13 | loss: 1.38793 | validation_logloss: 0.62498 | validation_accuracy: 0.76712 | validation_auc: 0.51381 | validation_balanced_accuracy: 0.50095 |  0:13:42s\n",
      "epoch 14 | loss: 1.3918  | validation_logloss: 0.70277 | validation_accuracy: 0.65458 | validation_auc: 0.52925 | validation_balanced_accuracy: 0.52035 |  0:14:15s\n",
      "epoch 15 | loss: 1.41393 | validation_logloss: 0.6419  | validation_accuracy: 0.72316 | validation_auc: 0.52679 | validation_balanced_accuracy: 0.51585 |  0:14:49s\n",
      "epoch 16 | loss: 1.34378 | validation_logloss: 0.92468 | validation_accuracy: 0.51688 | validation_auc: 0.51484 | validation_balanced_accuracy: 0.51182 |  0:15:23s\n",
      "epoch 17 | loss: 1.27856 | validation_logloss: 1.01492 | validation_accuracy: 0.43678 | validation_auc: 0.51065 | validation_balanced_accuracy: 0.50856 |  0:15:56s\n",
      "epoch 18 | loss: 1.26874 | validation_logloss: 0.95387 | validation_accuracy: 0.43107 | validation_auc: 0.50532 | validation_balanced_accuracy: 0.50913 |  0:16:30s\n",
      "epoch 19 | loss: 1.25157 | validation_logloss: 0.98562 | validation_accuracy: 0.41367 | validation_auc: 0.51896 | validation_balanced_accuracy: 0.51861 |  0:17:03s\n",
      "epoch 20 | loss: 1.22473 | validation_logloss: 0.94457 | validation_accuracy: 0.41869 | validation_auc: 0.51747 | validation_balanced_accuracy: 0.51422 |  0:17:37s\n",
      "epoch 21 | loss: 1.29594 | validation_logloss: 1.4054  | validation_accuracy: 0.22268 | validation_auc: 0.47234 | validation_balanced_accuracy: 0.48948 |  0:18:11s\n",
      "epoch 22 | loss: 1.27526 | validation_logloss: 1.07459 | validation_accuracy: 0.4268  | validation_auc: 0.48951 | validation_balanced_accuracy: 0.49475 |  0:18:44s\n",
      "epoch 23 | loss: 1.3058  | validation_logloss: 0.85347 | validation_accuracy: 0.6223  | validation_auc: 0.5155  | validation_balanced_accuracy: 0.50796 |  0:19:18s\n",
      "epoch 24 | loss: 1.31109 | validation_logloss: 1.65624 | validation_accuracy: 0.32343 | validation_auc: 0.50426 | validation_balanced_accuracy: 0.50845 |  0:19:51s\n",
      "epoch 25 | loss: 1.25555 | validation_logloss: 1.06432 | validation_accuracy: 0.51784 | validation_auc: 0.51881 | validation_balanced_accuracy: 0.51533 |  0:20:25s\n",
      "epoch 26 | loss: 1.20091 | validation_logloss: 0.884   | validation_accuracy: 0.60009 | validation_auc: 0.52951 | validation_balanced_accuracy: 0.51942 |  0:20:58s\n",
      "epoch 27 | loss: 1.25902 | validation_logloss: 0.78641 | validation_accuracy: 0.66236 | validation_auc: 0.54321 | validation_balanced_accuracy: 0.53009 |  0:21:32s\n",
      "epoch 28 | loss: 1.20068 | validation_logloss: 1.26078 | validation_accuracy: 0.30556 | validation_auc: 0.52329 | validation_balanced_accuracy: 0.51711 |  0:22:06s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFitting baseline TabNet 10 model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mtabnet_baseline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_base\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalidation\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlogloss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbalanced_accuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPATIENCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVIRTUAL_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweighted_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m end_time = time.time()\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBaseline model training completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:258\u001b[39m, in \u001b[36mTabModel.fit\u001b[39m\u001b[34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_epochs):\n\u001b[32m    254\u001b[39m \n\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28mself\u001b[39m._callback_container.on_epoch_begin(epoch_idx)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m     \u001b[38;5;66;03m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, valid_dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:489\u001b[39m, in \u001b[36mTabModel._train_epoch\u001b[39m\u001b[34m(self, train_loader)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[32m    487\u001b[39m     \u001b[38;5;28mself\u001b[39m._callback_container.on_batch_begin(batch_idx)\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     batch_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28mself\u001b[39m._callback_container.on_batch_end(batch_idx, batch_logs)\n\u001b[32m    493\u001b[39m epoch_logs = {\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._optimizer.param_groups[-\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:534\u001b[39m, in \u001b[36mTabModel._train_batch\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    531\u001b[39m loss = loss - \u001b[38;5;28mself\u001b[39m.lambda_sparse * M_loss\n\u001b[32m    533\u001b[39m \u001b[38;5;66;03m# Perform backward pass and optimization\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clip_value:\n\u001b[32m    536\u001b[39m     clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.network.parameters(), \u001b[38;5;28mself\u001b[39m.clip_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Initialize and Train a Baseline TabNet Model ---\n",
    "print(\"\\nTraining Baseline TabNet 10 Model without CV...\")\n",
    "\n",
    "TABNET_PARAMS = dict(\n",
    "    n_d=128, n_a=128, n_steps=7, n_independent=6, n_shared=6,\n",
    "    mask_type='sparsemax', gamma=1.5, lambda_sparse=0.0001, \n",
    "    optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=0.001),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    scheduler_params=dict(\n",
    "        mode=\"max\", patience=30, min_lr=1e-7, factor=0.05),\n",
    "    verbose=1, seed=86, device_name=device_name,\n",
    ")\n",
    "\n",
    "MAX_EPOCHS = 6000\n",
    "PATIENCE = 200\n",
    "BATCH_SIZE = 8192\n",
    "VIRTUAL_BATCH_SIZE = 2048\n",
    "\n",
    "X_train_base, X_val_base, y_train_base, y_val_base = train_test_split(\n",
    "    X_train_np, y_train, test_size=0.20, random_state=SEED, stratify=y_train\n",
    ")\n",
    "tabnet_baseline = TabNetClassifier(**TABNET_PARAMS)\n",
    "print(f\"TabNet 10 model initialized on {device_name}\")\n",
    "print(\"Fitting baseline TabNet 10 model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "tabnet_baseline.fit(\n",
    "    X_train=X_train_base, y_train=y_train_base,\n",
    "    eval_set=[(X_val_base, y_val_base)],\n",
    "    eval_name=['validation'],\n",
    "    eval_metric=['logloss', 'accuracy', 'auc', 'balanced_accuracy'],\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    virtual_batch_size=VIRTUAL_BATCH_SIZE,\n",
    "    drop_last=False,\n",
    "    loss_fn=weighted_loss,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Baseline model training completed in {(end_time - start_time):.2f} seconds.\")\n",
    "\n",
    "baseline_preds = tabnet_baseline.predict_proba(X_test_np)[:, 1]\n",
    "baseline_binary_preds = (baseline_preds > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n--- Baseline TabNet Model Evaluation ---\")\n",
    "baseline_results = evaluate_model(\n",
    "    y_test, baseline_preds, baseline_binary_preds, model_name=\"TabNet_10 (Baseline)\"\n",
    ")\n",
    "\n",
    "plot_roc_curve(y_test, baseline_preds, \"TabNet_10 (Baseline)\")\n",
    "\n",
    "# Save the model\n",
    "baseline_model_path = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_10_baseline_model\")\n",
    "saved_baseline_path = tabnet_baseline.save_model(baseline_model_path)\n",
    "print(f\"Baseline model saved to {saved_baseline_path}\")\n",
    "\n",
    "# Save baseline results\n",
    "baseline_results_df = pd.DataFrame([baseline_results])\n",
    "baseline_results_filename = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_10_baseline_results.csv\")\n",
    "baseline_results_df.to_csv(baseline_results_filename, index=False, mode='w+')\n",
    "print(f\"Baseline results saved to {baseline_results_filename}\")\n",
    "\n",
    "# Feature importance from baseline model\n",
    "baseline_importance = tabnet_baseline.feature_importances_\n",
    "baseline_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': baseline_importance})\n",
    "baseline_importance_df = baseline_importance_df.sort_values(by='Importance', ascending=False)\n",
    "baseline_importance_filename = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_10_baseline_feature_importance.csv\")\n",
    "baseline_importance_df.to_csv(baseline_importance_filename, index=False, mode='w+')\n",
    "print(f\"Baseline feature importance saved to {baseline_importance_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold based on Youden's J-Statistic (OOF): 0.5849\n",
      "Optimal threshold for validation set: 0.5849\n",
      "\n",
      "--- Baseline TabNet Model Evaluation with Optimal Threshold ---\n",
      "\n",
      "--- Evaluation Metrics for TabNet_Pro Max (Baseline with Optimal Threshold) ---\n",
      "AUC ROC:        0.7312\n",
      "Gini Coefficient: 0.4624\n",
      "KS Statistic:   0.3451\n",
      "Accuracy:       0.6895\n",
      "Precision:      0.1562\n",
      "Recall (TPR):   0.6467\n",
      "F1-Score:       0.2517\n",
      "Brier Score:    0.2553\n",
      "Log Loss:       0.7126\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39194 17343]\n",
      " [ 1754  3211]]\n",
      "Baseline results with optimal threshold saved to ./tabnet_outputs/tabnet_Pro Max_baseline_results_optimal.csv\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal threshold using the validation set\n",
    "optimal_threshold = find_optimal_threshold_j_statistic(y_val_base, tabnet_baseline.predict_proba(X_val_base)[:, 1])\n",
    "print(f\"Optimal threshold for validation set: {optimal_threshold:.4f}\")\n",
    "\n",
    "# Apply the optimal threshold to the test set predictions\n",
    "baseline_binary_preds_optimal = (baseline_preds > optimal_threshold).astype(int)\n",
    "# Evaluate the model with the optimal threshold\n",
    "print(\"\\n--- Baseline TabNet Model Evaluation with Optimal Threshold ---\")\n",
    "baseline_results_optimal = evaluate_model(\n",
    "    y_test, baseline_preds, baseline_binary_preds_optimal, model_name=\"TabNet_10 (Baseline with Optimal Threshold)\"\n",
    ")\n",
    "\n",
    "# Save the results with the optimal threshold\n",
    "baseline_results_optimal_df = pd.DataFrame([baseline_results_optimal])\n",
    "baseline_results_optimal_filename = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_10_baseline_results_optimal.csv\")\n",
    "baseline_results_optimal_df.to_csv(baseline_results_optimal_filename, index=False, mode='w+')\n",
    "print(f\"Baseline results with optimal threshold saved to {baseline_results_optimal_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
