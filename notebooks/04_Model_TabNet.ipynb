{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# --- Sklearn ---\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (roc_auc_score, roc_curve, precision_recall_curve, auc,\n",
    "                             accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, brier_score_loss, log_loss, classification_report)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# --- Imbalanced Learn ---\n",
    "from imblearn.over_sampling import SMOTE # Or other variants if preferred\n",
    "\n",
    "# --- TabNet & PyTorch ---\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Utility functions + Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions (can be imported from a utils file or redefined) ---\n",
    "def calculate_ks(y_true, y_prob):\n",
    "    \"\"\"Calculates the Kolmogorov-Smirnov (KS) statistic.\"\"\"\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_prob': y_prob})\n",
    "    df = df.sort_values(by='y_prob', ascending=False)\n",
    "    # Ensure y_true sums are not zero before division\n",
    "    sum_true = df['y_true'].sum()\n",
    "    sum_false = len(df) - sum_true\n",
    "    if sum_true == 0 or sum_false == 0:\n",
    "        return 0.0 # KS is 0 if one class is missing\n",
    "    df['cumulative_true'] = df['y_true'].cumsum() / sum_true\n",
    "    df['cumulative_false'] = (1 - df['y_true']).cumsum() / sum_false\n",
    "    ks = max(abs(df['cumulative_true'] - df['cumulative_false']))\n",
    "    return ks\n",
    "\n",
    "def find_optimal_threshold_j_statistic(y_true, y_prob_oof):\n",
    "    \"\"\"Finds the optimal threshold maximizing Youden's J statistic (Sensitivity + Specificity - 1).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob_oof)\n",
    "     # Handle cases where thresholds might not be strictly decreasing\n",
    "    valid_indices = np.where(np.isfinite(thresholds))[0]\n",
    "    if len(valid_indices) == 0:\n",
    "        print(\"Warning: No valid thresholds found for J-statistic calculation.\")\n",
    "        return 0.5 # Default fallback\n",
    "    fpr, tpr, thresholds = fpr[valid_indices], tpr[valid_indices], thresholds[valid_indices]\n",
    "\n",
    "    if len(thresholds) == 0:\n",
    "         print(\"Warning: Threshold array is empty after filtering.\")\n",
    "         return 0.5\n",
    "\n",
    "    j_statistic = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_statistic)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    # Ensure threshold is within [0, 1] bounds if necessary due to floating point issues\n",
    "    optimal_threshold = max(0.0, min(1.0, optimal_threshold))\n",
    "    print(f\"Optimal threshold based on Youden's J-Statistic (OOF): {optimal_threshold:.4f}\")\n",
    "    return optimal_threshold\n",
    "\n",
    "def evaluate_model(y_true, y_pred_proba, y_pred_binary, model_name=\"Model\"):\n",
    "    \"\"\"Calculates and prints standard classification metrics.\"\"\"\n",
    "    # Add epsilon to probabilities for log_loss if necessary\n",
    "    eps = 1e-15\n",
    "    y_pred_proba = np.clip(y_pred_proba, eps, 1 - eps)\n",
    "\n",
    "    auc_roc = roc_auc_score(y_true, y_pred_proba)\n",
    "    gini = 2 * auc_roc - 1\n",
    "    ks = calculate_ks(y_true, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "    precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "    brier = brier_score_loss(y_true, y_pred_proba)\n",
    "    logloss = log_loss(y_true, y_pred_proba)\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "\n",
    "    print(f\"\\n--- Evaluation Metrics for {model_name} ---\")\n",
    "    print(f\"AUC ROC:        {auc_roc:.4f}\")\n",
    "    print(f\"Gini Coefficient: {gini:.4f}\")\n",
    "    print(f\"KS Statistic:   {ks:.4f}\")\n",
    "    print(f\"Accuracy:       {accuracy:.4f}\")\n",
    "    print(f\"Precision:      {precision:.4f}\")\n",
    "    print(f\"Recall (TPR):   {recall:.4f}\")\n",
    "    print(f\"F1-Score:       {f1:.4f}\")\n",
    "    print(f\"Brier Score:    {brier:.4f}\")\n",
    "    print(f\"Log Loss:       {logloss:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'AUC': auc_roc, 'Gini': gini, 'KS': ks, 'Accuracy': accuracy,\n",
    "        'Precision': precision, 'Recall': recall, 'F1': f1,\n",
    "        'Brier': brier, 'LogLoss': logloss\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def plot_roc_curve(y_true, y_prob, model_name):\n",
    "    \"\"\"Plots the ROC curve.\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    auc_roc = roc_auc_score(y_true, y_prob)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_roc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # Diagonal line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} - ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # Save the plot\n",
    "    plot_filename = f\"roc_curve_{model_name.replace(' ', '_')}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"ROC curve saved to {plot_filename}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "DATA_PATH = '../data/processed/'\n",
    "MODEL_OUTPUT_PATH = './tabnet_outputs/' # Directory to save model/results\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "SEED = 42\n",
    "N_SPLITS = 5 # Number of folds for Cross-Validation\n",
    "SMOTE_STRATEGY = 0.5 # Ratio after resampling\n",
    "TARGET = 'TARGET'\n",
    "ID_COL = 'SK_ID_CURR'\n",
    "\n",
    "# TabNet Specific Config\n",
    "# These are EXAMPLE parameters, tuning is recommended\n",
    "TABNET_PARAMS = dict(\n",
    "    # Network architecture\n",
    "    n_d=64,              # Increase from 50 to capture more complex patterns\n",
    "    n_a=64,              # Match n_d for balanced attention mechanism\n",
    "    n_steps=7,           # Increase from 3 for deeper feature processing\n",
    "    n_independent=2,     # Add independent layer count (wasn't in original)\n",
    "    n_shared=2,          # Add shared layer count (wasn't in original)\n",
    "    \n",
    "    # Regularization\n",
    "    gamma=1.3,           # Slightly increased feature reuse penalty\n",
    "    lambda_sparse=5e-4,  # Increased sparsity for better feature selection\n",
    "    \n",
    "    # Optimizer settings\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=5e-3),  # Lower learning rate\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler_params=dict(\n",
    "        mode=\"min\", \n",
    "        patience=10,      # Increased patience \n",
    "        min_lr=1e-5, \n",
    "        factor=0.5\n",
    "    ),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    \n",
    "    mask_type='sparsemax',\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Training Config\n",
    "MAX_EPOCHS = 200         # Increased to allow more training time\n",
    "PATIENCE = 20            # Increased early stopping patience\n",
    "BATCH_SIZE = 4096*2        # Larger batch size for efficiency (adjust based on GPU memory)\n",
    "VIRTUAL_BATCH_SIZE = 512*2 # Increased for better batch normalization\n",
    "\n",
    "# --- Check for GPU ---\n",
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device_name}\")\n",
    "TABNET_PARAMS['device_name'] = device_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Data loaded successfully.\n",
      "\n",
      "Applying Low Variance Feature Selection...\n",
      "Original number of features: 773\n",
      "Number of features after variance thresholding: 542\n",
      "Updated X_train shape: (246005, 542)\n",
      "Updated X_test shape: (61502, 542)\n",
      "WARNING: Treating all features as numerical for TabNet due to pre-encoded input data.\n",
      "Prepared X_train shape: (246005, 542)\n",
      "Prepared X_test shape: (61502, 542)\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data ---\n",
    "print(\"Loading preprocessed data...\")\n",
    "try:\n",
    "    train_df = pd.read_csv(DATA_PATH + 'train_final.csv')\n",
    "    test_df = pd.read_csv(DATA_PATH + 'test_final.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Make sure 'train_final.csv' and 'test_final.csv' are in {DATA_PATH}\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Data ---\n",
    "y_train = train_df[TARGET].values # Use .values for numpy arrays\n",
    "y_test = test_df[TARGET].values\n",
    "\n",
    "# Drop Target and potentially ID\n",
    "if ID_COL in train_df.columns:\n",
    "    X_train = train_df.drop(columns=[TARGET, ID_COL])\n",
    "    X_test = test_df.drop(columns=[TARGET, ID_COL])\n",
    "else:\n",
    "     X_train = train_df.drop(columns=[TARGET])\n",
    "     X_test = test_df.drop(columns=[TARGET])\n",
    "\n",
    "# Align columns just in case\n",
    "common_cols = list(X_train.columns.intersection(X_test.columns))\n",
    "X_train = X_train[common_cols]\n",
    "X_test = X_test[common_cols]\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "\n",
    "# Feature Selection using variance threshold (optional)\n",
    "# --- Feature Selection: Low Variance Filter ---\n",
    "print(\"\\nApplying Low Variance Feature Selection...\")\n",
    "var_selector = VarianceThreshold(threshold=0.01) # Threshold=0 removes zero-variance, 0.01 removes low variance\n",
    "\n",
    "# Fit on training data only\n",
    "var_selector.fit(X_train)\n",
    "\n",
    "# Get the boolean mask of selected features\n",
    "feature_mask = var_selector.get_support()\n",
    "original_feature_names = X_train.columns.tolist() # Get original names before transformation\n",
    "selected_feature_names = [name for name, selected in zip(original_feature_names, feature_mask) if selected]\n",
    "\n",
    "print(f\"Original number of features: {X_train.shape[1]}\")\n",
    "print(f\"Number of features after variance thresholding: {len(selected_feature_names)}\")\n",
    "\n",
    "# Transform both X_train and X_test\n",
    "X_train_np_selected = var_selector.transform(X_train)\n",
    "X_test_np_selected = var_selector.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame with selected column names\n",
    "X_train = pd.DataFrame(X_train_np_selected, columns=selected_feature_names, index=X_train.index)\n",
    "X_test = pd.DataFrame(X_test_np_selected, columns=selected_feature_names, index=X_test.index)\n",
    "\n",
    "# Update the global feature_names list\n",
    "feature_names = selected_feature_names\n",
    "\n",
    "print(f\"Updated X_train shape: {X_train.shape}\")\n",
    "print(f\"Updated X_test shape: {X_test.shape}\")\n",
    "\n",
    "# --- LIMITATION: Treat all features as numerical ---\n",
    "# Ideally, identify original categorical features and pass their indices to TabNet.\n",
    "# Since we are using pre-encoded data, we treat all as numerical.\n",
    "print(\"WARNING: Treating all features as numerical for TabNet due to pre-encoded input data.\")\n",
    "categorical_indices = [] # No categorical indices provided\n",
    "categorical_dims = [] # No specific dimensions needed if indices are empty\n",
    "\n",
    "# Convert to numpy arrays of type float32 for PyTorch\n",
    "X_train_np = X_train.replace([np.inf, -np.inf], np.nan).fillna(X_train.median()).astype(np.float32).values\n",
    "X_test_np = X_test.replace([np.inf, -np.inf], np.nan).fillna(X_train.median()).astype(np.float32).values # Use train median\n",
    "\n",
    "print(f\"Prepared X_train shape: {X_train_np.shape}\")\n",
    "print(f\"Prepared X_test shape: {X_test_np.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class weights: [ 1.       11.386959] to emphasize minority class\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights inversely proportional to class frequencies\n",
    "class_counts = np.bincount(y_train)\n",
    "total_samples = len(y_train)\n",
    "# More aggressive weighting for minority class\n",
    "class_weights = torch.tensor([1.0, (class_counts[0]/class_counts[1]) * 1.0], dtype=torch.float32)\n",
    "if device_name == 'cuda':\n",
    "    class_weights = class_weights.cuda()\n",
    "\n",
    "print(f\"Using class weights: {class_weights.cpu().numpy()} to emphasize minority class\")\n",
    "\n",
    "# Create weighted loss function\n",
    "weighted_loss = torch.nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== GPU Diagnostics =====\n",
      "CUDA Version: 12.6\n",
      "PyTorch Version: 2.6.0+cu126\n",
      "Successfully allocated tensor on GPU, Memory used: 573.81 MB\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n",
      "GPU Memory: 12.00 GB\n",
      "Compute Capability: 8.6\n",
      "Memory released\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "# Add this before the training loop (just before or after the cell with id \"fe7d1abd\")\n",
    "\n",
    "# Verify GPU is properly detected and can allocate memory\n",
    "if device_name == 'cuda':\n",
    "    print(\"===== GPU Diagnostics =====\")\n",
    "    # Print CUDA version\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    # Create a sample tensor on GPU to verify memory allocation works\n",
    "    test_tensor = torch.zeros((1000, 1000), device='cuda')\n",
    "    allocated_memory = torch.cuda.memory_allocated() / (1024**2)\n",
    "    print(f\"Successfully allocated tensor on GPU, Memory used: {allocated_memory:.2f} MB\")\n",
    "    \n",
    "    # Get device properties\n",
    "    device_properties = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU Name: {device_properties.name}\")\n",
    "    print(f\"GPU Memory: {device_properties.total_memory / (1024**3):.2f} GB\")\n",
    "    print(f\"Compute Capability: {device_properties.major}.{device_properties.minor}\")\n",
    "    \n",
    "    # Free memory\n",
    "    del test_tensor\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Memory released\")\n",
    "    \n",
    "    print(\"===========================\")\n",
    "else:\n",
    "    print(\"GPU not available, running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting TabNet 5-Fold Cross-Validation...\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Applying StandardScaler...\n",
      "Moving data to GPU...\n",
      "GPU Memory in use after data transfer: 1.0559 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device check - will use GPU: True\n",
      "Training TabNet model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000024E83BC37E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"c:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1576, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.08314 | validation_auc: 0.52639 |  0:00:04s\n",
      "epoch 1  | loss: 0.85214 | validation_auc: 0.58793 |  0:00:08s\n",
      "epoch 2  | loss: 0.77767 | validation_auc: 0.63648 |  0:00:13s\n",
      "epoch 3  | loss: 0.70323 | validation_auc: 0.67678 |  0:00:17s\n",
      "epoch 4  | loss: 0.67241 | validation_auc: 0.69399 |  0:00:22s\n",
      "epoch 5  | loss: 0.65604 | validation_auc: 0.69298 |  0:00:26s\n",
      "epoch 6  | loss: 0.64267 | validation_auc: 0.69778 |  0:00:31s\n",
      "epoch 7  | loss: 0.64687 | validation_auc: 0.70464 |  0:00:35s\n",
      "epoch 8  | loss: 0.63256 | validation_auc: 0.70818 |  0:00:40s\n",
      "epoch 9  | loss: 0.62917 | validation_auc: 0.7136  |  0:00:44s\n",
      "epoch 10 | loss: 0.62813 | validation_auc: 0.70638 |  0:00:48s\n",
      "epoch 11 | loss: 0.63655 | validation_auc: 0.71648 |  0:00:53s\n",
      "epoch 12 | loss: 0.62619 | validation_auc: 0.71478 |  0:00:57s\n",
      "epoch 13 | loss: 0.6251  | validation_auc: 0.7165  |  0:01:02s\n",
      "epoch 14 | loss: 0.62674 | validation_auc: 0.71918 |  0:01:06s\n",
      "epoch 15 | loss: 0.61719 | validation_auc: 0.71866 |  0:01:11s\n",
      "epoch 16 | loss: 0.61904 | validation_auc: 0.7214  |  0:01:15s\n",
      "epoch 17 | loss: 0.61641 | validation_auc: 0.71986 |  0:01:19s\n",
      "epoch 18 | loss: 0.61602 | validation_auc: 0.7213  |  0:01:24s\n",
      "epoch 19 | loss: 0.61556 | validation_auc: 0.72146 |  0:01:28s\n",
      "epoch 20 | loss: 0.617   | validation_auc: 0.71425 |  0:01:33s\n",
      "epoch 21 | loss: 0.61805 | validation_auc: 0.71996 |  0:01:37s\n",
      "epoch 22 | loss: 0.61722 | validation_auc: 0.71902 |  0:01:41s\n",
      "epoch 23 | loss: 0.61656 | validation_auc: 0.71958 |  0:01:46s\n",
      "epoch 24 | loss: 0.61582 | validation_auc: 0.71845 |  0:01:50s\n",
      "epoch 25 | loss: 0.61491 | validation_auc: 0.72198 |  0:01:55s\n",
      "epoch 26 | loss: 0.61548 | validation_auc: 0.72102 |  0:01:59s\n",
      "epoch 27 | loss: 0.61461 | validation_auc: 0.72215 |  0:02:04s\n",
      "epoch 28 | loss: 0.61385 | validation_auc: 0.72128 |  0:02:08s\n",
      "epoch 29 | loss: 0.61458 | validation_auc: 0.72084 |  0:02:13s\n",
      "epoch 30 | loss: 0.61378 | validation_auc: 0.71904 |  0:02:17s\n",
      "epoch 31 | loss: 0.6136  | validation_auc: 0.719   |  0:02:21s\n",
      "epoch 32 | loss: 0.61285 | validation_auc: 0.72013 |  0:02:26s\n",
      "epoch 33 | loss: 0.61383 | validation_auc: 0.72006 |  0:02:30s\n",
      "epoch 34 | loss: 0.61307 | validation_auc: 0.72003 |  0:02:35s\n",
      "epoch 35 | loss: 0.61281 | validation_auc: 0.72084 |  0:02:39s\n",
      "epoch 36 | loss: 0.61369 | validation_auc: 0.72055 |  0:02:44s\n",
      "epoch 37 | loss: 0.6133  | validation_auc: 0.72248 |  0:02:48s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining TabNet model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# 5. Use the tensors directly for training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_train_fold_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_val_fold_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m              \u001b[49m\u001b[43my_val_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_val_fold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalidation\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use AUC for early stopping metric\u001b[39;49;00m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPATIENCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVIRTUAL_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to 0 when using GPU to avoid multiprocessing issues\u001b[39;49;00m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Important for final batch\u001b[39;49;00m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweighted_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use weighted loss instead of standard CrossEntropyLoss\u001b[39;49;00m\n\u001b[32m     66\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# 6. Predict on Validation and Test Sets\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPredicting on validation and test sets...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:258\u001b[39m, in \u001b[36mTabModel.fit\u001b[39m\u001b[34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_epochs):\n\u001b[32m    254\u001b[39m \n\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28mself\u001b[39m._callback_container.on_epoch_begin(epoch_idx)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m     \u001b[38;5;66;03m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, valid_dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:489\u001b[39m, in \u001b[36mTabModel._train_epoch\u001b[39m\u001b[34m(self, train_loader)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[32m    487\u001b[39m     \u001b[38;5;28mself\u001b[39m._callback_container.on_batch_begin(batch_idx)\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     batch_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28mself\u001b[39m._callback_container.on_batch_end(batch_idx, batch_logs)\n\u001b[32m    493\u001b[39m epoch_logs = {\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._optimizer.param_groups[-\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:534\u001b[39m, in \u001b[36mTabModel._train_batch\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    531\u001b[39m loss = loss - \u001b[38;5;28mself\u001b[39m.lambda_sparse * M_loss\n\u001b[32m    533\u001b[39m \u001b[38;5;66;03m# Perform backward pass and optimization\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clip_value:\n\u001b[32m    536\u001b[39m     clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.network.parameters(), \u001b[38;5;28mself\u001b[39m.clip_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ORLab\\main_source\\CreditRiskProject\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Cross-Validation Loop ---\n",
    "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "oof_predictions = np.zeros(X_train_np.shape[0])\n",
    "test_predictions_list = []\n",
    "fold_models = []\n",
    "fold_results = []\n",
    "\n",
    "print(f\"\\nStarting TabNet {N_SPLITS}-Fold Cross-Validation...\")\n",
    "start_cv_time = time.time()\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_np, y_train)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    fold_start_time = time.time()\n",
    "\n",
    "    # 1. Split data for the fold\n",
    "    X_train_fold, X_val_fold = X_train_np[train_idx], X_train_np[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    # 3. Apply Scaling (Fit on Resampled Train, Transform Train & Val)\n",
    "    print(\"Applying StandardScaler...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "    X_val_fold_scaled = scaler.transform(X_val_fold) \n",
    "\n",
    "    # 4. Define and Train TabNet Model for the fold\n",
    "    model = TabNetClassifier(**TABNET_PARAMS)\n",
    "\n",
    "    print(\"Training TabNet model...\")\n",
    "\n",
    "    model.fit(\n",
    "        X_train=X_train_fold_scaled, y_train=y_train_fold,\n",
    "        eval_set=[(X_val_fold_scaled, y_val_fold)],\n",
    "        eval_name=['validation'],\n",
    "        eval_metric=['auc'], # Use AUC for early stopping metric\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        patience=PATIENCE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        virtual_batch_size=VIRTUAL_BATCH_SIZE,\n",
    "        num_workers=0, # Adjust based on system\n",
    "        drop_last=False, # Important for final batch\n",
    "        loss_fn=weighted_loss # Use weighted loss instead of standard CrossEntropyLoss\n",
    "    )\n",
    "\n",
    "    # 5. Predict on Validation and Test Sets\n",
    "    print(\"Predicting on validation and test sets...\")\n",
    "    val_preds = model.predict_proba(X_val_fold_scaled)[:, 1]\n",
    "    # Scale the full test set using the scaler fitted for this fold\n",
    "    X_test_scaled = scaler.transform(X_test_np)\n",
    "    test_preds = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # 6. Store Predictions\n",
    "    oof_predictions[val_idx] = val_preds\n",
    "    test_predictions_list.append(test_preds)\n",
    "    fold_models.append(model) # Store the model if needed\n",
    "\n",
    "    # 7. Evaluate Fold (optional)\n",
    "    fold_auc = roc_auc_score(y_val_fold, val_preds)\n",
    "    print(f\"Fold {fold+1} Validation AUC: {fold_auc:.4f}\")\n",
    "    fold_results.append({'Fold': fold+1, 'Validation AUC': fold_auc})\n",
    "\n",
    "    fold_end_time = time.time()\n",
    "    print(f\"Fold {fold+1} completed in {(fold_end_time - fold_start_time):.2f} seconds.\")\n",
    "\n",
    "end_cv_time = time.time()\n",
    "print(f\"\\nCross-Validation finished in {(end_cv_time - start_cv_time)/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aggregate and Evaluate ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Evaluation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Average test predictions across folds\n",
    "final_test_predictions = np.mean(test_predictions_list, axis=0)\n",
    "\n",
    "# Evaluate OOF predictions\n",
    "oof_auc = roc_auc_score(y_train, oof_predictions)\n",
    "print(f\"Overall OOF AUC: {oof_auc:.4f}\")\n",
    "\n",
    "# Find optimal threshold using OOF predictions\n",
    "optimal_threshold = find_optimal_threshold_j_statistic(y_train, oof_predictions)\n",
    "\n",
    "# Evaluate final test predictions using the optimal threshold\n",
    "final_test_predictions_binary = (final_test_predictions >= optimal_threshold).astype(int)\n",
    "final_results = evaluate_model(y_test, final_test_predictions, final_test_predictions_binary, \"TabNet (Tuned CV)\")\n",
    "\n",
    "# Plot ROC curve for the averaged test predictions\n",
    "plot_roc_curve(y_test, final_test_predictions, \"TabNet (Tuned CV)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Optuna Hyperparameter Optimization for TabNet ---\n",
    "# print(\"\\n--- Optuna Optimization for TabNet ---\")\n",
    "# import optuna\n",
    "# from optuna.pruners import MedianPruner\n",
    "\n",
    "# # Define the objective function for Optuna\n",
    "# def objective_tabnet(trial):\n",
    "#     # Define hyperparameters to tune\n",
    "#     n_d = trial.suggest_int('n_d', 8, 128)\n",
    "#     n_a = trial.suggest_int('n_a', 8, 128)\n",
    "#     n_steps = trial.suggest_int('n_steps', 3, 10)\n",
    "#     n_independent = trial.suggest_int('n_independent', 1, 5)\n",
    "#     n_shared = trial.suggest_int('n_shared', 1, 5)\n",
    "#     gamma = trial.suggest_float('gamma', 1.0, 2.0)\n",
    "#     lambda_sparse = trial.suggest_float('lambda_sparse', 1e-6, 1e-1, log=True)\n",
    "#     learning_rate = trial.suggest_float('learning_rate', 5e-4, 2e-2, log=True)\n",
    "#     batch_size = trial.suggest_categorical('batch_size', [1024, 2048, 4096, 8192])\n",
    "#     virtual_batch_size = trial.suggest_int('virtual_batch_size', 128, 1024, log=True)\n",
    "    \n",
    "#     # Create the TabNet model with suggested parameters\n",
    "#     params = dict(\n",
    "#         n_d=n_d, \n",
    "#         n_a=n_a,\n",
    "#         n_steps=n_steps,\n",
    "#         n_independent=n_independent,\n",
    "#         n_shared=n_shared,\n",
    "#         gamma=gamma,\n",
    "#         lambda_sparse=lambda_sparse,\n",
    "#         optimizer_fn=torch.optim.Adam,\n",
    "#         optimizer_params=dict(lr=learning_rate),\n",
    "#         scheduler_params=dict(mode=\"min\", patience=7, min_lr=1e-5, factor=0.5),\n",
    "#         scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "#         mask_type='sparsemax',\n",
    "#         verbose=0,  # Reduced verbosity for optimization\n",
    "#         seed=SEED,\n",
    "#         device_name=device_name\n",
    "#     )\n",
    "    \n",
    "#     # Cross-validation setup\n",
    "#     cv_scores = []\n",
    "#     fold_models = []\n",
    "    \n",
    "#     for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_np, y_train)):\n",
    "#         X_train_fold, X_val_fold = X_train_np[train_idx], X_train_np[val_idx]\n",
    "#         y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "#         # Apply scaling\n",
    "#         scaler = StandardScaler()\n",
    "#         X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "#         X_val_fold_scaled = scaler.transform(X_val_fold)\n",
    "        \n",
    "#         # Train TabNet model\n",
    "#         model = TabNetClassifier(**params)\n",
    "        \n",
    "#         model.fit(\n",
    "#             X_train=X_train_fold_scaled, \n",
    "#             y_train=y_train_fold,\n",
    "#             eval_set=[(X_val_fold_scaled, y_val_fold)],\n",
    "#             eval_name=['validation'],\n",
    "#             eval_metric=['auc'],\n",
    "#             max_epochs=MAX_EPOCHS,\n",
    "#             patience=PATIENCE,\n",
    "#             batch_size=batch_size,\n",
    "#             virtual_batch_size=virtual_batch_size,\n",
    "#             num_workers=0\n",
    "#         )\n",
    "        \n",
    "#         # Get validation AUC\n",
    "#         val_preds = model.predict_proba(X_val_fold_scaled)[:, 1]\n",
    "#         fold_auc = roc_auc_score(y_val_fold, val_preds)\n",
    "#         cv_scores.append(fold_auc)\n",
    "        \n",
    "#         # Report intermediate value for pruning\n",
    "#         trial.report(fold_auc, fold)\n",
    "        \n",
    "#         # Pruning check\n",
    "#         if trial.should_prune():\n",
    "#             raise optuna.TrialPruned()\n",
    "    \n",
    "#     # Return mean CV score\n",
    "#     return np.mean(cv_scores)\n",
    "\n",
    "# # Create Optuna study with pruning\n",
    "# OPTUNA_N_TRIALS = 20  # Adjust as needed based on your computational resources\n",
    "# storage_path = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_optuna_studies.db\")\n",
    "# study = optuna.create_study(\n",
    "#     direction='maximize',\n",
    "#     study_name='tabnet_optimization',\n",
    "#     storage=f'sqlite:///{storage_path}',\n",
    "#     load_if_exists=True,\n",
    "#     pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    "# )\n",
    "\n",
    "# # Run optimization\n",
    "# print(f\"Starting Optuna optimization for TabNet with {OPTUNA_N_TRIALS} trials...\")\n",
    "# start_time = time.time()\n",
    "# study.optimize(objective_tabnet, n_trials=OPTUNA_N_TRIALS, n_jobs=1)\n",
    "# end_time = time.time()\n",
    "# print(f\"Optuna optimization completed in {(end_time - start_time)/60:.2f} minutes\")\n",
    "\n",
    "# # Get best parameters\n",
    "# best_params = study.best_params\n",
    "# print(f\"\\nBest Params (TabNet): {best_params}\")\n",
    "# print(f\"Best CV AUC score: {study.best_value:.4f}\")\n",
    "\n",
    "# # Create final model with best parameters\n",
    "# final_tabnet_params = dict(\n",
    "#     n_d=best_params['n_d'],\n",
    "#     n_a=best_params['n_a'],\n",
    "#     n_steps=best_params['n_steps'],\n",
    "#     n_independent=best_params['n_independent'],\n",
    "#     n_shared=best_params['n_shared'],\n",
    "#     gamma=best_params['gamma'],\n",
    "#     lambda_sparse=best_params['lambda_sparse'],\n",
    "#     optimizer_fn=torch.optim.Adam,\n",
    "#     optimizer_params=dict(lr=best_params['learning_rate']),\n",
    "#     scheduler_params=dict(mode=\"min\", patience=7, min_lr=1e-5, factor=0.5),\n",
    "#     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "#     mask_type='sparsemax',\n",
    "#     verbose=1,\n",
    "#     seed=SEED,\n",
    "#     device_name=device_name\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-train with the best parameters on the full training set\n",
    "# print(\"\\nRetraining TabNet with best parameters on the full training set...\")\n",
    "\n",
    "# # Apply scaling to the full training data\n",
    "# scaler = StandardScaler()\n",
    "# X_train_np_scaled = scaler.fit_transform(X_train_np)\n",
    "# X_test_np_scaled = scaler.transform(X_test_np)\n",
    "\n",
    "# # Split a portion for validation to monitor during training\n",
    "# X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "#     X_train_np_scaled, y_train, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# final_model = TabNetClassifier(**final_tabnet_params)\n",
    "# final_model.fit(\n",
    "#     X_train=X_train_final, \n",
    "#     y_train=y_train_final,\n",
    "#     eval_set=[(X_val_final, y_val_final)],\n",
    "#     eval_name=['validation'],\n",
    "#     eval_metric=['auc'],\n",
    "#     max_epochs=MAX_EPOCHS,\n",
    "#     patience=PATIENCE,\n",
    "#     batch_size=best_params.get('batch_size', BATCH_SIZE),\n",
    "#     virtual_batch_size=best_params.get('virtual_batch_size', VIRTUAL_BATCH_SIZE),\n",
    "#     num_workers=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Importance ---\n",
    "# TabNet provides feature importance based on the masks used in its attention mechanism\n",
    "print(\"\\n--- TabNet Feature Importances (from last fold model) ---\")\n",
    "try:\n",
    "    # Importance shape is (n_features,)\n",
    "    importances = fold_models[-1].feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
    "    display(feature_importance_df.head(50)) # Show top 50\n",
    "    # Save importances\n",
    "    importance_filename = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_feature_importances.csv\")\n",
    "    feature_importance_df.to_csv(importance_filename, index=False)\n",
    "    print(f\"Feature importances saved to {importance_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get or save feature importances: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save Results ---\n",
    "print(\"\\nSaving results...\")\n",
    "results_summary = pd.DataFrame([final_results]).set_index('Model')\n",
    "summary_filename = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_evaluation_summary.csv\")\n",
    "results_summary.to_csv(summary_filename)\n",
    "print(f\"Evaluation summary saved to {summary_filename}\")\n",
    "\n",
    "# Save OOF and Test predictions\n",
    "oof_df = pd.DataFrame({'SK_ID_CURR': train_df.index, 'oof_pred_proba': oof_predictions}) # Assuming train_df index maps correctly\n",
    "oof_filename = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_oof_predictions.csv\")\n",
    "oof_df.to_csv(oof_filename, index=False)\n",
    "print(f\"OOF predictions saved to {oof_filename}\")\n",
    "\n",
    "test_pred_df = pd.DataFrame({'SK_ID_CURR': test_df.index, 'test_pred_proba': final_test_predictions}) # Assuming test_df index maps correctly\n",
    "test_pred_filename = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_test_predictions.csv\")\n",
    "test_pred_df.to_csv(test_pred_filename, index=False)\n",
    "print(f\"Test predictions saved to {test_pred_filename}\")\n",
    "\n",
    "# Optionally save one of the trained models (e.g., the last fold's)\n",
    "# Note: Saving/loading TabNet models might require saving the associated zip file.\n",
    "model_save_path = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_model_last_fold\")\n",
    "saved_path = fold_models[-1].save_model(model_save_path)\n",
    "print(f\"Last fold TabNet model saved to path: {saved_path}\")\n",
    "\n",
    "print(\"\\nTabNet script finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
