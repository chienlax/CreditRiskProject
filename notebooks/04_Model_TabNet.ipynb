{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# --- Sklearn ---\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (roc_auc_score, roc_curve, precision_recall_curve, auc,\n",
    "                             accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, brier_score_loss, log_loss, classification_report)\n",
    "\n",
    "# --- Imbalanced Learn ---\n",
    "from imblearn.over_sampling import SMOTE # Or other variants if preferred\n",
    "\n",
    "# --- TabNet & PyTorch ---\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Utility functions + Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions (can be imported from a utils file or redefined) ---\n",
    "def calculate_ks(y_true, y_prob):\n",
    "    \"\"\"Calculates the Kolmogorov-Smirnov (KS) statistic.\"\"\"\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_prob': y_prob})\n",
    "    df = df.sort_values(by='y_prob', ascending=False)\n",
    "    # Ensure y_true sums are not zero before division\n",
    "    sum_true = df['y_true'].sum()\n",
    "    sum_false = len(df) - sum_true\n",
    "    if sum_true == 0 or sum_false == 0:\n",
    "        return 0.0 # KS is 0 if one class is missing\n",
    "    df['cumulative_true'] = df['y_true'].cumsum() / sum_true\n",
    "    df['cumulative_false'] = (1 - df['y_true']).cumsum() / sum_false\n",
    "    ks = max(abs(df['cumulative_true'] - df['cumulative_false']))\n",
    "    return ks\n",
    "\n",
    "def find_optimal_threshold_j_statistic(y_true, y_prob_oof):\n",
    "    \"\"\"Finds the optimal threshold maximizing Youden's J statistic (Sensitivity + Specificity - 1).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob_oof)\n",
    "     # Handle cases where thresholds might not be strictly decreasing\n",
    "    valid_indices = np.where(np.isfinite(thresholds))[0]\n",
    "    if len(valid_indices) == 0:\n",
    "        print(\"Warning: No valid thresholds found for J-statistic calculation.\")\n",
    "        return 0.5 # Default fallback\n",
    "    fpr, tpr, thresholds = fpr[valid_indices], tpr[valid_indices], thresholds[valid_indices]\n",
    "\n",
    "    if len(thresholds) == 0:\n",
    "         print(\"Warning: Threshold array is empty after filtering.\")\n",
    "         return 0.5\n",
    "\n",
    "    j_statistic = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_statistic)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    # Ensure threshold is within [0, 1] bounds if necessary due to floating point issues\n",
    "    optimal_threshold = max(0.0, min(1.0, optimal_threshold))\n",
    "    print(f\"Optimal threshold based on Youden's J-Statistic (OOF): {optimal_threshold:.4f}\")\n",
    "    return optimal_threshold\n",
    "\n",
    "def evaluate_model(y_true, y_pred_proba, y_pred_binary, model_name=\"Model\"):\n",
    "    \"\"\"Calculates and prints standard classification metrics.\"\"\"\n",
    "    # Add epsilon to probabilities for log_loss if necessary\n",
    "    eps = 1e-15\n",
    "    y_pred_proba = np.clip(y_pred_proba, eps, 1 - eps)\n",
    "\n",
    "    auc_roc = roc_auc_score(y_true, y_pred_proba)\n",
    "    gini = 2 * auc_roc - 1\n",
    "    ks = calculate_ks(y_true, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "    precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "    brier = brier_score_loss(y_true, y_pred_proba)\n",
    "    logloss = log_loss(y_true, y_pred_proba)\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "\n",
    "    print(f\"\\n--- Evaluation Metrics for {model_name} ---\")\n",
    "    print(f\"AUC ROC:        {auc_roc:.4f}\")\n",
    "    print(f\"Gini Coefficient: {gini:.4f}\")\n",
    "    print(f\"KS Statistic:   {ks:.4f}\")\n",
    "    print(f\"Accuracy:       {accuracy:.4f}\")\n",
    "    print(f\"Precision:      {precision:.4f}\")\n",
    "    print(f\"Recall (TPR):   {recall:.4f}\")\n",
    "    print(f\"F1-Score:       {f1:.4f}\")\n",
    "    print(f\"Brier Score:    {brier:.4f}\")\n",
    "    print(f\"Log Loss:       {logloss:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'AUC': auc_roc, 'Gini': gini, 'KS': ks, 'Accuracy': accuracy,\n",
    "        'Precision': precision, 'Recall': recall, 'F1': f1,\n",
    "        'Brier': brier, 'LogLoss': logloss\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def plot_roc_curve(y_true, y_prob, model_name):\n",
    "    \"\"\"Plots the ROC curve.\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    auc_roc = roc_auc_score(y_true, y_prob)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_roc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # Diagonal line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} - ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # Save the plot\n",
    "    plot_filename = f\"roc_curve_{model_name.replace(' ', '_')}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"ROC curve saved to {plot_filename}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DATA_PATH = '../data/processed/'\n",
    "MODEL_OUTPUT_PATH = './tabnet_outputs/' # Directory to save model/results\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "SEED = 42\n",
    "N_SPLITS = 5 # Number of folds for Cross-Validation\n",
    "SMOTE_STRATEGY = 0.5 # Ratio after resampling\n",
    "TARGET = 'TARGET'\n",
    "ID_COL = 'SK_ID_CURR'\n",
    "\n",
    "# TabNet Specific Config\n",
    "# These are EXAMPLE parameters, tuning is recommended\n",
    "TABNET_PARAMS = dict(\n",
    "    n_d=24, n_a=24, # Dimension of prediction/attention layers (adjust based on feature count/memory)\n",
    "    n_steps=3,      # Number of steps in the architecture\n",
    "    gamma=1.3,      # Coefficient for feature reusage penalty\n",
    "    lambda_sparse=1e-4, # Sparsity loss coefficient\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2), # Learning rate (often needs tuning)\n",
    "    scheduler_params=dict(mode=\"min\", patience=5, min_lr=1e-5, factor=0.5), # ReduceLROnPlateau scheduler\n",
    "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    mask_type='sparsemax', # Attention mechanism type ('sparsemax' or 'entmax')\n",
    "    verbose=10,      # Print loss every 10 epochs\n",
    "    seed=SEED\n",
    ")\n",
    "# Training Config\n",
    "MAX_EPOCHS = 100     # Max epochs per fold\n",
    "PATIENCE = 15       # Early stopping patience\n",
    "BATCH_SIZE = 2048   # Adjust based on GPU memory\n",
    "VIRTUAL_BATCH_SIZE = 256 # Used if BATCH_SIZE is small\n",
    "\n",
    "# --- Check for GPU ---\n",
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device_name}\")\n",
    "TABNET_PARAMS['device_name'] = device_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Data ---\n",
    "print(\"Loading preprocessed data...\")\n",
    "try:\n",
    "    train_df = pd.read_csv(DATA_PATH + 'train_final.csv')\n",
    "    test_df = pd.read_csv(DATA_PATH + 'test_final.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Make sure 'train_final.csv' and 'test_final.csv' are in {DATA_PATH}\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Data ---\n",
    "y_train = train_df[TARGET].values # Use .values for numpy arrays\n",
    "y_test = test_df[TARGET].values\n",
    "\n",
    "# Drop Target and potentially ID\n",
    "if ID_COL in train_df.columns:\n",
    "    X_train = train_df.drop(columns=[TARGET, ID_COL])\n",
    "    X_test = test_df.drop(columns=[TARGET, ID_COL])\n",
    "else:\n",
    "     X_train = train_df.drop(columns=[TARGET])\n",
    "     X_test = test_df.drop(columns=[TARGET])\n",
    "\n",
    "# Align columns just in case\n",
    "common_cols = list(X_train.columns.intersection(X_test.columns))\n",
    "X_train = X_train[common_cols]\n",
    "X_test = X_test[common_cols]\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# --- LIMITATION: Treat all features as numerical ---\n",
    "# Ideally, identify original categorical features and pass their indices to TabNet.\n",
    "# Since we are using pre-encoded data, we treat all as numerical.\n",
    "print(\"WARNING: Treating all features as numerical for TabNet due to pre-encoded input data.\")\n",
    "categorical_indices = [] # No categorical indices provided\n",
    "categorical_dims = [] # No specific dimensions needed if indices are empty\n",
    "\n",
    "# Convert to numpy arrays of type float32 for PyTorch\n",
    "X_train_np = X_train.replace([np.inf, -np.inf], np.nan).fillna(X_train.median()).astype(np.float32).values\n",
    "X_test_np = X_test.replace([np.inf, -np.inf], np.nan).fillna(X_train.median()).astype(np.float32).values # Use train median\n",
    "\n",
    "print(f\"Prepared X_train shape: {X_train_np.shape}\")\n",
    "print(f\"Prepared X_test shape: {X_test_np.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cross-Validation Loop ---\n",
    "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "oof_predictions = np.zeros(X_train_np.shape[0])\n",
    "test_predictions_list = []\n",
    "fold_models = []\n",
    "fold_results = []\n",
    "\n",
    "print(f\"\\nStarting TabNet {N_SPLITS}-Fold Cross-Validation...\")\n",
    "start_cv_time = time.time()\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_np, y_train)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    fold_start_time = time.time()\n",
    "\n",
    "    # 1. Split data for the fold\n",
    "    X_train_fold, X_val_fold = X_train_np[train_idx], X_train_np[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    # 2. Apply SMOTE to the training part of the fold\n",
    "    print(\"Applying SMOTE...\")\n",
    "    smote = SMOTE(sampling_strategy=SMOTE_STRATEGY, random_state=SEED + fold, n_jobs=-1) # Vary seed per fold\n",
    "    try:\n",
    "        X_train_fold_res, y_train_fold_res = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "        print(f\"SMOTE applied. Original size: {X_train_fold.shape[0]}, Resampled size: {X_train_fold_res.shape[0]}\")\n",
    "    except Exception as e:\n",
    "         print(f\"SMOTE failed for fold {fold+1}: {e}. Using original data.\")\n",
    "         X_train_fold_res, y_train_fold_res = X_train_fold, y_train_fold # Fallback\n",
    "\n",
    "    # 3. Apply Scaling (Fit on Resampled Train, Transform Train & Val)\n",
    "    print(\"Applying StandardScaler...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_fold_scaled = scaler.fit_transform(X_train_fold_res)\n",
    "    X_val_fold_scaled = scaler.transform(X_val_fold) # Use same scaler fitted on train\n",
    "\n",
    "    # 4. Define and Train TabNet Model for the fold\n",
    "    model = TabNetClassifier(**TABNET_PARAMS)\n",
    "\n",
    "    print(\"Training TabNet model...\")\n",
    "    model.fit(\n",
    "        X_train=X_train_fold_scaled, y_train=y_train_fold_res,\n",
    "        eval_set=[(X_val_fold_scaled, y_val_fold)],\n",
    "        eval_name=['validation'],\n",
    "        eval_metric=['auc'], # Use AUC for early stopping metric\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        patience=PATIENCE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        virtual_batch_size=VIRTUAL_BATCH_SIZE,\n",
    "        num_workers=0, # Adjust based on system\n",
    "        drop_last=False, # Important for final batch\n",
    "        loss_fn=torch.nn.CrossEntropyLoss() # Standard loss for binary classification\n",
    "    )\n",
    "\n",
    "    # 5. Predict on Validation and Test Sets\n",
    "    print(\"Predicting on validation and test sets...\")\n",
    "    val_preds = model.predict_proba(X_val_fold_scaled)[:, 1]\n",
    "    # Scale the full test set using the scaler fitted for this fold\n",
    "    X_test_scaled = scaler.transform(X_test_np)\n",
    "    test_preds = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # 6. Store Predictions\n",
    "    oof_predictions[val_idx] = val_preds\n",
    "    test_predictions_list.append(test_preds)\n",
    "    fold_models.append(model) # Store the model if needed\n",
    "\n",
    "    # 7. Evaluate Fold (optional)\n",
    "    fold_auc = roc_auc_score(y_val_fold, val_preds)\n",
    "    print(f\"Fold {fold+1} Validation AUC: {fold_auc:.4f}\")\n",
    "    fold_results.append({'Fold': fold+1, 'Validation AUC': fold_auc})\n",
    "\n",
    "    fold_end_time = time.time()\n",
    "    print(f\"Fold {fold+1} completed in {(fold_end_time - fold_start_time):.2f} seconds.\")\n",
    "\n",
    "end_cv_time = time.time()\n",
    "print(f\"\\nCross-Validation finished in {(end_cv_time - start_cv_time)/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aggregate and Evaluate ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Evaluation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Average test predictions across folds\n",
    "final_test_predictions = np.mean(test_predictions_list, axis=0)\n",
    "\n",
    "# Evaluate OOF predictions\n",
    "oof_auc = roc_auc_score(y_train, oof_predictions)\n",
    "print(f\"Overall OOF AUC: {oof_auc:.4f}\")\n",
    "\n",
    "# Find optimal threshold using OOF predictions\n",
    "optimal_threshold = find_optimal_threshold_j_statistic(y_train, oof_predictions)\n",
    "\n",
    "# Evaluate final test predictions using the optimal threshold\n",
    "final_test_predictions_binary = (final_test_predictions >= optimal_threshold).astype(int)\n",
    "final_results = evaluate_model(y_test, final_test_predictions, final_test_predictions_binary, \"TabNet (Tuned CV)\")\n",
    "\n",
    "# Plot ROC curve for the averaged test predictions\n",
    "plot_roc_curve(y_test, final_test_predictions, \"TabNet (Tuned CV)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Importance ---\n",
    "# TabNet provides feature importance based on the masks used in its attention mechanism\n",
    "print(\"\\n--- TabNet Feature Importances (from last fold model) ---\")\n",
    "try:\n",
    "    # Importance shape is (n_features,)\n",
    "    importances = fold_models[-1].feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
    "    display(feature_importance_df.head(50)) # Show top 50\n",
    "    # Save importances\n",
    "    importance_filename = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_feature_importances.csv\")\n",
    "    feature_importance_df.to_csv(importance_filename, index=False)\n",
    "    print(f\"Feature importances saved to {importance_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get or save feature importances: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save Results ---\n",
    "print(\"\\nSaving results...\")\n",
    "results_summary = pd.DataFrame([final_results]).set_index('Model')\n",
    "summary_filename = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_evaluation_summary.csv\")\n",
    "results_summary.to_csv(summary_filename)\n",
    "print(f\"Evaluation summary saved to {summary_filename}\")\n",
    "\n",
    "# Save OOF and Test predictions\n",
    "oof_df = pd.DataFrame({'SK_ID_CURR': train_df.index, 'oof_pred_proba': oof_predictions}) # Assuming train_df index maps correctly\n",
    "oof_filename = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_oof_predictions.csv\")\n",
    "oof_df.to_csv(oof_filename, index=False)\n",
    "print(f\"OOF predictions saved to {oof_filename}\")\n",
    "\n",
    "test_pred_df = pd.DataFrame({'SK_ID_CURR': test_df.index, 'test_pred_proba': final_test_predictions}) # Assuming test_df index maps correctly\n",
    "test_pred_filename = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_test_predictions.csv\")\n",
    "test_pred_df.to_csv(test_pred_filename, index=False)\n",
    "print(f\"Test predictions saved to {test_pred_filename}\")\n",
    "\n",
    "# Optionally save one of the trained models (e.g., the last fold's)\n",
    "# Note: Saving/loading TabNet models might require saving the associated zip file.\n",
    "# model_save_path = os.path.join(MODEL_OUTPUT_PATH, \"tabnet_model_last_fold\")\n",
    "# saved_path = fold_models[-1].save_model(model_save_path)\n",
    "# print(f\"Last fold TabNet model saved to path: {saved_path}\")\n",
    "\n",
    "print(\"\\nTabNet script finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
