{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Scikit-learn for preprocessing and modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer # Using SimpleImputer instead of KNN/XGB for EXT_SOURCE initially for simplicity, can swap later\n",
    "from sklearn.preprocessing import OneHotEncoder # For Bureau categoricals\n",
    "from xgboost import XGBRegressor # Keep for EXT_SOURCE imputation as in original code\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option('future.no_silent_downcasting', True) # Future warning for downcasting, can be useful to catch potential issues\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # Suppress SettingWithCopyWarning for cleaner output, use .copy() where needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Loading Functions ---\n",
    "\n",
    "def load_initial_data(data_path):\n",
    "    \"\"\"Loads the required raw data files.\"\"\"\n",
    "    print(\"Loading raw data...\")\n",
    "    try:\n",
    "        app_train_df = pd.read_csv(data_path + 'application_train.csv')\n",
    "        bureau_df = pd.read_csv(data_path + 'bureau.csv')\n",
    "        bureau_balance_df = pd.read_csv(data_path + 'bureau_balance.csv')\n",
    "        \n",
    "        # Ensure bureau_df only contains SK_ID_CURR in app_train_df\n",
    "        bureau_df = bureau_df[bureau_df['SK_ID_CURR'].isin(app_train_df['SK_ID_CURR'])]\n",
    "        bureau_balance_df = bureau_balance_df[bureau_balance_df['SK_ID_BUREAU'].isin(bureau_df['SK_ID_BUREAU'])]\n",
    "        \n",
    "        print(\"Raw data loaded successfully.\")\n",
    "        return app_train_df, bureau_df, bureau_balance_df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading files: {e}. Make sure files are in {data_path}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Application Data Preprocessing & Feature Engineering Functions ---\n",
    "\n",
    "def split_application_data(app_df, target_col='TARGET', test_size=0.2, random_state=42):\n",
    "    \"\"\"Splits the application data into training and testing sets.\"\"\"\n",
    "    print(f\"Splitting application data (Test size: {test_size}, Random State: {random_state})...\")\n",
    "    X = app_df.drop(columns=[target_col])\n",
    "    y = app_df[target_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def clean_application(df):\n",
    "    \"\"\"Applies basic cleaning steps to application data (train or test).\"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    print(f\"Applying basic cleaning to DataFrame with shape: {df_cleaned.shape}\")\n",
    "\n",
    "    # Drop specified FLAG_DOCUMENT columns (low variance)\n",
    "    flag_cols_to_drop = ['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_20']\n",
    "    # Ensure columns exist before dropping\n",
    "    flag_cols_to_drop = [col for col in flag_cols_to_drop if col in df_cleaned.columns]\n",
    "    df_cleaned = df_cleaned.drop(flag_cols_to_drop, axis=1)\n",
    "    print(f\"Dropped low-variance FLAG columns: {flag_cols_to_drop}\")\n",
    "\n",
    "    # Convert DAYS_BIRTH to years (positive)\n",
    "    if 'DAYS_BIRTH' in df_cleaned.columns:\n",
    "        df_cleaned['DAYS_BIRTH'] = df_cleaned['DAYS_BIRTH'] * -1 / 365\n",
    "        print(\"Converted DAYS_BIRTH to years.\")\n",
    "\n",
    "    # Handle DAYS_EMPLOYED anomaly (365243 -> NaN)\n",
    "    if 'DAYS_EMPLOYED' in df_cleaned.columns:\n",
    "        df_cleaned['DAYS_EMPLOYED'] = df_cleaned['DAYS_EMPLOYED'].replace(365243, np.nan)\n",
    "        print(\"Replaced DAYS_EMPLOYED anomaly (365243) with NaN.\")\n",
    "\n",
    "    # Handle OBS_..._SOCIAL_CIRCLE anomalies (> 30 -> NaN) - Corrected assignment\n",
    "    # Ensure these columns exist before attempting replacement\n",
    "    if 'OBS_30_CNT_SOCIAL_CIRCLE' in df_cleaned.columns:\n",
    "        df_cleaned.loc[df_cleaned['OBS_30_CNT_SOCIAL_CIRCLE'] > 30, 'OBS_30_CNT_SOCIAL_CIRCLE'] = np.nan\n",
    "        print(\"Capped OBS_30_CNT_SOCIAL_CIRCLE at 30 (values > 30 set to NaN).\")\n",
    "    if 'OBS_60_CNT_SOCIAL_CIRCLE' in df_cleaned.columns:\n",
    "        df_cleaned.loc[df_cleaned['OBS_60_CNT_SOCIAL_CIRCLE'] > 30, 'OBS_60_CNT_SOCIAL_CIRCLE'] = np.nan\n",
    "        print(\"Capped OBS_60_CNT_SOCIAL_CIRCLE at 30 (values > 30 set to NaN).\")\n",
    "\n",
    "    # Note: Removing 'XNA' Gender was in the original code.\n",
    "    # It's generally better to handle this *before* splitting if it's a clear data error,\n",
    "    # or handle it as another category ('XNA') during encoding if it might be meaningful.\n",
    "    # We'll keep the original behavior but apply it *before* the split for simplicity here.\n",
    "    # (See `preprocess_credit_data` function where this is called before split)\n",
    "\n",
    "    # Convert REGION_RATING columns to object type (as per original logic, for later encoding)\n",
    "    # This assumes they will be treated as categorical later (e.g., by Response Encoding)\n",
    "    if 'REGION_RATING_CLIENT' in df_cleaned.columns:\n",
    "        df_cleaned['REGION_RATING_CLIENT'] = df_cleaned['REGION_RATING_CLIENT'].astype('object')\n",
    "        print(\"Converted REGION_RATING_CLIENT to object type.\")\n",
    "    if 'REGION_RATING_CLIENT_W_CITY' in df_cleaned.columns:\n",
    "        df_cleaned['REGION_RATING_CLIENT_W_CITY'] = df_cleaned['REGION_RATING_CLIENT_W_CITY'].astype('object')\n",
    "        print(\"Converted REGION_RATING_CLIENT_W_CITY to object type.\")\n",
    "\n",
    "    print(\"Basic cleaning finished.\")\n",
    "    return df_cleaned\n",
    "\n",
    "def handle_app_missing_categorical(X_train, X_test):\n",
    "    \"\"\"Fills missing values in categorical columns with 'XNA'.\"\"\"\n",
    "    print(\"Handling missing categorical values (filling with 'XNA')...\")\n",
    "    X_train_filled = X_train.copy()\n",
    "    X_test_filled = X_test.copy()\n",
    "\n",
    "    categorical_cols = X_train_filled.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "    X_train_filled[categorical_cols] = X_train_filled[categorical_cols].fillna('XNA')\n",
    "    X_test_filled[categorical_cols] = X_test_filled[categorical_cols].fillna('XNA')\n",
    "\n",
    "    print(f\"Filled NaNs in {len(categorical_cols)} categorical columns.\")\n",
    "    return X_train_filled, X_test_filled\n",
    "\n",
    "def impute_ext_sources_xgb(X_train, X_test):\n",
    "    \"\"\"Imputes missing EXT_SOURCE features using XGBoost Regressor.\n",
    "       Fits on train data, transforms both train and test.\"\"\"\n",
    "    print(\"Imputing EXT_SOURCE columns using XGBoost...\")\n",
    "    X_train_imputed = X_train.copy()\n",
    "    X_test_imputed = X_test.copy()\n",
    "\n",
    "    # Identify numeric columns available for prediction (excluding target and IDs)\n",
    "    numeric_cols = X_train_imputed.select_dtypes(include=np.number).columns.tolist()\n",
    "    exclude_cols = ['SK_ID_CURR', 'TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']\n",
    "    cols_for_modelling_base = [col for col in numeric_cols if col not in exclude_cols]\n",
    "\n",
    "    # Impute in order of least missing to most missing\n",
    "    for ext_col in ['EXT_SOURCE_2', 'EXT_SOURCE_3', 'EXT_SOURCE_1']:\n",
    "        if ext_col not in X_train_imputed.columns:\n",
    "            print(f\"Column {ext_col} not found, skipping imputation.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Imputing {ext_col}...\")\n",
    "        # Define current set of predictor columns\n",
    "        cols_for_modelling = [col for col in cols_for_modelling_base if col in X_train_imputed.columns]\n",
    "        # Add previously imputed EXT_SOURCE cols if they exist\n",
    "        prev_ext_cols = [c for c in ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3'] if c != ext_col and c in X_train_imputed.columns and X_train_imputed[c].isnull().sum() == 0]\n",
    "        cols_for_modelling.extend(prev_ext_cols)\n",
    "        cols_for_modelling = list(set(cols_for_modelling)) # Ensure uniqueness\n",
    "\n",
    "        # Prepare data for this specific column imputation\n",
    "        train_not_missing = X_train_imputed[X_train_imputed[ext_col].notna()]\n",
    "        train_missing = X_train_imputed[X_train_imputed[ext_col].isna()]\n",
    "        test_missing = X_test_imputed[X_test_imputed[ext_col].isna()]\n",
    "\n",
    "        if train_missing.empty and test_missing.empty:\n",
    "             print(f\"No missing values found for {ext_col}. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        # Only proceed if there are non-missing values to train on\n",
    "        if not train_not_missing.empty:\n",
    "            X_model_train = train_not_missing[cols_for_modelling].fillna(train_not_missing[cols_for_modelling].median()) # Handle potential NaNs in predictors\n",
    "            Y_model_train = train_not_missing[ext_col]\n",
    "\n",
    "            # Define and train the model\n",
    "            xg = XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=-1, random_state=59, objective='reg:squarederror')\n",
    "            xg.fit(X_model_train, Y_model_train)\n",
    "            print(f\"Trained XGBoost model for {ext_col}.\")\n",
    "\n",
    "            # Impute missing values in train set\n",
    "            if not train_missing.empty:\n",
    "                X_pred_train = train_missing[cols_for_modelling].fillna(train_not_missing[cols_for_modelling].median()) # Use median from non-missing train part\n",
    "                X_train_imputed.loc[X_train_imputed[ext_col].isna(), ext_col] = xg.predict(X_pred_train)\n",
    "                print(f\"Imputed {train_missing.shape[0]} values in train set for {ext_col}.\")\n",
    "\n",
    "            # Impute missing values in test set\n",
    "            if not test_missing.empty:\n",
    "                X_pred_test = test_missing[cols_for_modelling].fillna(train_not_missing[cols_for_modelling].median()) # Use median from non-missing train part\n",
    "                X_test_imputed.loc[X_test_imputed[ext_col].isna(), ext_col] = xg.predict(X_pred_test)\n",
    "                print(f\"Imputed {test_missing.shape[0]} values in test set for {ext_col}.\")\n",
    "        else:\n",
    "             print(f\"Warning: No non-missing values found for {ext_col} in training data. Cannot train imputer. Filling with global median.\")\n",
    "             global_median = X_train_imputed[ext_col].median() # Fallback: should not happen if called in order\n",
    "             X_train_imputed[ext_col] = X_train_imputed[ext_col].fillna(global_median)\n",
    "             X_test_imputed[ext_col] = X_test_imputed[ext_col].fillna(global_median)\n",
    "\n",
    "\n",
    "    print(\"Finished EXT_SOURCE imputation.\")\n",
    "    return X_train_imputed, X_test_imputed\n",
    "\n",
    "def engineer_app_numeric_features(df):\n",
    "    \"\"\"Engineers numeric features based on domain knowledge for application data.\"\"\"\n",
    "    eng_df = df.copy()\n",
    "    print(f\"Applying numeric feature engineering to DataFrame with shape: {eng_df.shape}\")\n",
    "\n",
    "    # Convert REGION_RATING back to numeric for calculations if they exist and are object type\n",
    "    if 'REGION_RATING_CLIENT' in eng_df.columns and eng_df['REGION_RATING_CLIENT'].dtype == 'object':\n",
    "        # Attempt conversion, coercing errors to NaN (which might be handled later or indicate issues)\n",
    "        eng_df['REGION_RATING_CLIENT'] = pd.to_numeric(eng_df['REGION_RATING_CLIENT'], errors='coerce')\n",
    "    if 'REGION_RATING_CLIENT_W_CITY' in eng_df.columns and eng_df['REGION_RATING_CLIENT_W_CITY'].dtype == 'object':\n",
    "        eng_df['REGION_RATING_CLIENT_W_CITY'] = pd.to_numeric(eng_df['REGION_RATING_CLIENT_W_CITY'], errors='coerce')\n",
    "\n",
    "    # Define epsilon for safe division\n",
    "    eps = 1e-5\n",
    "\n",
    "    # Income / Credit Ratios\n",
    "    # Ensure required columns exist before creating features\n",
    "    if 'AMT_CREDIT' in eng_df.columns and 'AMT_INCOME_TOTAL' in eng_df.columns:\n",
    "        eng_df['CREDIT_INCOME_RATIO'] = eng_df['AMT_CREDIT'] / (eng_df['AMT_INCOME_TOTAL'] + eps)\n",
    "    if 'AMT_CREDIT' in eng_df.columns and 'AMT_ANNUITY' in eng_df.columns:\n",
    "        eng_df['CREDIT_ANNUITY_RATIO'] = eng_df['AMT_CREDIT'] / (eng_df['AMT_ANNUITY'] + eps)\n",
    "    if 'AMT_ANNUITY' in eng_df.columns and 'AMT_INCOME_TOTAL' in eng_df.columns:\n",
    "        eng_df['ANNUITY_INCOME_RATIO'] = eng_df['AMT_ANNUITY'] / (eng_df['AMT_INCOME_TOTAL'] + eps)\n",
    "        eng_df['INCOME_ANNUITY_DIFF'] = eng_df['AMT_INCOME_TOTAL'] - eng_df['AMT_ANNUITY']\n",
    "    if 'AMT_CREDIT' in eng_df.columns and 'AMT_GOODS_PRICE' in eng_df.columns:\n",
    "        eng_df['CREDIT_GOODS_RATIO'] = eng_df['AMT_CREDIT'] / (eng_df['AMT_GOODS_PRICE'] + eps)\n",
    "        eng_df['CREDIT_GOODS_DIFF'] = eng_df['AMT_CREDIT'] - eng_df['AMT_GOODS_PRICE']\n",
    "    if 'AMT_GOODS_PRICE' in eng_df.columns and 'AMT_INCOME_TOTAL' in eng_df.columns:\n",
    "        eng_df['GOODS_INCOME_RATIO'] = eng_df['AMT_GOODS_PRICE'] / (eng_df['AMT_INCOME_TOTAL'] + eps)\n",
    "    if 'AMT_INCOME_TOTAL' in eng_df.columns and 'EXT_SOURCE_3' in eng_df.columns:\n",
    "         eng_df['INCOME_EXT_RATIO'] = eng_df['AMT_INCOME_TOTAL'] / (eng_df['EXT_SOURCE_3'] + eps)\n",
    "    if 'AMT_CREDIT' in eng_df.columns and 'EXT_SOURCE_3' in eng_df.columns:\n",
    "         eng_df['CREDIT_EXT_RATIO'] = eng_df['AMT_CREDIT'] / (eng_df['EXT_SOURCE_3'] + eps)\n",
    "\n",
    "    # Age / Employment Ratios\n",
    "    if 'DAYS_BIRTH' in eng_df.columns and 'DAYS_EMPLOYED' in eng_df.columns:\n",
    "        # Note: DAYS_BIRTH is now positive years. DAYS_EMPLOYED might be negative days or NaN.\n",
    "        # To make comparable, convert DAYS_EMPLOYED to positive years if not NaN.\n",
    "        days_employed_years = eng_df['DAYS_EMPLOYED'].apply(lambda x: -x / 365 if pd.notna(x) else np.nan)\n",
    "        eng_df['AGE_EMPLOYED_DIFF'] = eng_df['DAYS_BIRTH'] - days_employed_years # Diff in years\n",
    "        eng_df['EMPLOYED_TO_AGE_RATIO'] = days_employed_years / (eng_df['DAYS_BIRTH'] + eps)\n",
    "\n",
    "    # Car Ratios\n",
    "    if 'OWN_CAR_AGE' in eng_df.columns and 'DAYS_EMPLOYED' in eng_df.columns:\n",
    "        days_employed_years = eng_df['DAYS_EMPLOYED'].apply(lambda x: -x / 365 if pd.notna(x) else np.nan)\n",
    "        eng_df['CAR_EMPLOYED_DIFF'] = eng_df['OWN_CAR_AGE'] - days_employed_years\n",
    "        eng_df['CAR_EMPLOYED_RATIO'] = eng_df['OWN_CAR_AGE'] / (days_employed_years + eps)\n",
    "    if 'DAYS_BIRTH' in eng_df.columns and 'OWN_CAR_AGE' in eng_df.columns:\n",
    "        eng_df['CAR_AGE_DIFF'] = eng_df['DAYS_BIRTH'] - eng_df['OWN_CAR_AGE']\n",
    "        eng_df['CAR_AGE_RATIO'] = eng_df['OWN_CAR_AGE'] / (eng_df['DAYS_BIRTH'] + eps)\n",
    "\n",
    "    # Flag Contacts Sum\n",
    "    contact_flags = ['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL']\n",
    "    existing_contact_flags = [flag for flag in contact_flags if flag in eng_df.columns]\n",
    "    if existing_contact_flags:\n",
    "        eng_df['FLAG_CONTACTS_SUM'] = eng_df[existing_contact_flags].sum(axis=1)\n",
    "\n",
    "    # Hour Process * Credit\n",
    "    if 'AMT_CREDIT' in eng_df.columns and 'HOUR_APPR_PROCESS_START' in eng_df.columns:\n",
    "        eng_df['HOUR_PROCESS_CREDIT_MUL'] = eng_df['AMT_CREDIT'] * eng_df['HOUR_APPR_PROCESS_START']\n",
    "\n",
    "    # Family Members\n",
    "    if 'CNT_FAM_MEMBERS' in eng_df.columns and 'CNT_CHILDREN' in eng_df.columns:\n",
    "        eng_df['CNT_NON_CHILDREN'] = eng_df['CNT_FAM_MEMBERS'] - eng_df['CNT_CHILDREN']\n",
    "    if 'CNT_CHILDREN' in eng_df.columns and 'AMT_INCOME_TOTAL' in eng_df.columns:\n",
    "        eng_df['CHILDREN_INCOME_RATIO'] = eng_df['CNT_CHILDREN'] / (eng_df['AMT_INCOME_TOTAL'] + eps)\n",
    "    if 'AMT_INCOME_TOTAL' in eng_df.columns and 'CNT_FAM_MEMBERS' in eng_df.columns:\n",
    "        eng_df['PER_CAPITA_INCOME'] = eng_df['AMT_INCOME_TOTAL'] / (eng_df['CNT_FAM_MEMBERS'] + 1) # Add 1 to avoid division by zero if CNT_FAM_MEMBERS is 0\n",
    "\n",
    "    # Region Ratings (assuming they are numeric now)\n",
    "    if 'REGION_RATING_CLIENT' in eng_df.columns and 'REGION_RATING_CLIENT_W_CITY' in eng_df.columns and 'AMT_INCOME_TOTAL' in eng_df.columns:\n",
    "         eng_df['REGIONS_RATING_INCOME_MUL'] = (eng_df['REGION_RATING_CLIENT'] + eng_df['REGION_RATING_CLIENT_W_CITY']) * eng_df['AMT_INCOME_TOTAL'] / 2\n",
    "         # Use .max(axis=1) and .min(axis=1) for row-wise operations\n",
    "         eng_df['REGION_RATING_MAX'] = eng_df[['REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY']].max(axis=1)\n",
    "         # Original code had a typo assigning min to max. Corrected:\n",
    "         eng_df['REGION_RATING_MIN'] = eng_df[['REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY']].min(axis=1)\n",
    "         eng_df['REGION_RATING_MEAN'] = eng_df[['REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY']].mean(axis=1)\n",
    "         eng_df['REGION_RATING_MUL'] = eng_df['REGION_RATING_CLIENT'] * eng_df['REGION_RATING_CLIENT_W_CITY']\n",
    "\n",
    "    # Flag Regions Sum\n",
    "    region_flags = ['REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION',\n",
    "                    'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY']\n",
    "    existing_region_flags = [flag for flag in region_flags if flag in eng_df.columns]\n",
    "    if existing_region_flags:\n",
    "        eng_df['FLAG_REGIONS_SUM'] = eng_df[existing_region_flags].sum(axis=1)\n",
    "\n",
    "    # External Sources Aggregations\n",
    "    ext_source_cols = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']\n",
    "    existing_ext_sources = [col for col in ext_source_cols if col in eng_df.columns]\n",
    "    if len(existing_ext_sources) > 0:\n",
    "        eng_df['EXT_SOURCE_MEAN'] = eng_df[existing_ext_sources].mean(axis=1)\n",
    "        eng_df['EXT_SOURCE_STD'] = eng_df[existing_ext_sources].std(axis=1)\n",
    "        eng_df['EXT_SOURCE_MAX'] = eng_df[existing_ext_sources].max(axis=1)\n",
    "        eng_df['EXT_SOURCE_MIN'] = eng_df[existing_ext_sources].min(axis=1)\n",
    "        # Original multiplication might lead to very small/large numbers or zero if one source is missing/zero\n",
    "        # eng_df['EXT_SOURCE_MUL'] = eng_df[existing_ext_sources].prod(axis=1) # Use prod for multiplication\n",
    "        # Weighted sum might be more robust\n",
    "        weights = {'EXT_SOURCE_1': 2, 'EXT_SOURCE_2': 3, 'EXT_SOURCE_3': 4}\n",
    "        weighted_sum = pd.Series(0, index=eng_df.index, dtype=float)\n",
    "        for col in existing_ext_sources:\n",
    "            weighted_sum += eng_df[col].fillna(0) * weights.get(col, 1) # Fill NaN with 0 for sum\n",
    "        eng_df['WEIGHTED_EXT_SOURCE'] = weighted_sum\n",
    "\n",
    "\n",
    "    # Apartment Scores Aggregations\n",
    "    # Define groups of apartment features\n",
    "    apartment_groups = {\n",
    "        'AVG': ['APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG'],\n",
    "        'MODE': ['APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'TOTALAREA_MODE'],\n",
    "        'MEDI': ['APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI']\n",
    "    }\n",
    "    for suffix, cols in apartment_groups.items():\n",
    "        existing_cols = [col for col in cols if col in eng_df.columns]\n",
    "        if existing_cols:\n",
    "            eng_df[f'APARTMENTS_SUM_{suffix}'] = eng_df[existing_cols].sum(axis=1)\n",
    "            if f'APARTMENTS_SUM_{suffix}' in eng_df.columns and 'AMT_INCOME_TOTAL' in eng_df.columns:\n",
    "                eng_df[f'INCOME_APARTMENT_{suffix}_MUL'] = eng_df[f'APARTMENTS_SUM_{suffix}'] * eng_df['AMT_INCOME_TOTAL']\n",
    "\n",
    "\n",
    "    # OBS and DEF Social Circle Features\n",
    "    if 'OBS_30_CNT_SOCIAL_CIRCLE' in eng_df.columns and 'OBS_60_CNT_SOCIAL_CIRCLE' in eng_df.columns:\n",
    "        eng_df['OBS_30_60_SUM'] = eng_df['OBS_30_CNT_SOCIAL_CIRCLE'] + eng_df['OBS_60_CNT_SOCIAL_CIRCLE']\n",
    "    if 'DEF_30_CNT_SOCIAL_CIRCLE' in eng_df.columns and 'DEF_60_CNT_SOCIAL_CIRCLE' in eng_df.columns:\n",
    "        eng_df['DEF_30_60_SUM'] = eng_df['DEF_30_CNT_SOCIAL_CIRCLE'] + eng_df['DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "    if 'OBS_30_CNT_SOCIAL_CIRCLE' in eng_df.columns and 'DEF_30_CNT_SOCIAL_CIRCLE' in eng_df.columns:\n",
    "        eng_df['OBS_DEF_30_MUL'] = eng_df['OBS_30_CNT_SOCIAL_CIRCLE'] * eng_df['DEF_30_CNT_SOCIAL_CIRCLE']\n",
    "    if 'OBS_60_CNT_SOCIAL_CIRCLE' in eng_df.columns and 'DEF_60_CNT_SOCIAL_CIRCLE' in eng_df.columns:\n",
    "        eng_df['OBS_DEF_60_MUL'] = eng_df['OBS_60_CNT_SOCIAL_CIRCLE'] * eng_df['DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "    obs_def_cols = ['OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "    existing_obs_def = [col for col in obs_def_cols if col in eng_df.columns]\n",
    "    if existing_obs_def:\n",
    "        eng_df['SUM_OBS_DEF_ALL'] = eng_df[existing_obs_def].sum(axis=1)\n",
    "    if 'AMT_CREDIT' in eng_df.columns:\n",
    "        if 'OBS_30_CNT_SOCIAL_CIRCLE' in eng_df.columns:\n",
    "             eng_df['OBS_30_CREDIT_RATIO'] = eng_df['AMT_CREDIT'] / (eng_df['OBS_30_CNT_SOCIAL_CIRCLE'] + eps)\n",
    "        if 'OBS_60_CNT_SOCIAL_CIRCLE' in eng_df.columns:\n",
    "             eng_df['OBS_60_CREDIT_RATIO'] = eng_df['AMT_CREDIT'] / (eng_df['OBS_60_CNT_SOCIAL_CIRCLE'] + eps)\n",
    "        if 'DEF_30_CNT_SOCIAL_CIRCLE' in eng_df.columns:\n",
    "             eng_df['DEF_30_CREDIT_RATIO'] = eng_df['AMT_CREDIT'] / (eng_df['DEF_30_CNT_SOCIAL_CIRCLE'] + eps)\n",
    "        if 'DEF_60_CNT_SOCIAL_CIRCLE' in eng_df.columns:\n",
    "             eng_df['DEF_60_CREDIT_RATIO'] = eng_df['AMT_CREDIT'] / (eng_df['DEF_60_CNT_SOCIAL_CIRCLE'] + eps)\n",
    "\n",
    "    # Flag Documents Sum (excluding those dropped earlier)\n",
    "    flag_cols_to_drop = ['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_20']\n",
    "    doc_flags = [f'FLAG_DOCUMENT_{i}' for i in range(3, 22) if f'FLAG_DOCUMENT_{i}' not in flag_cols_to_drop] # Original range was up to 21\n",
    "    existing_doc_flags = [flag for flag in doc_flags if flag in eng_df.columns]\n",
    "    if existing_doc_flags:\n",
    "        eng_df['SUM_FLAGS_DOCUMENTS'] = eng_df[existing_doc_flags].sum(axis=1)\n",
    "\n",
    "    # Details Change Features\n",
    "    days_change_cols = ['DAYS_LAST_PHONE_CHANGE', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH']\n",
    "    existing_days_change = [col for col in days_change_cols if col in eng_df.columns]\n",
    "    if len(existing_days_change) > 0:\n",
    "        # Multiplication might be sensitive to large negative numbers. Sum is safer.\n",
    "        # eng_df['DAYS_DETAILS_CHANGE_MUL'] = eng_df[existing_days_change].prod(axis=1)\n",
    "        eng_df['DAYS_DETAILS_CHANGE_SUM'] = eng_df[existing_days_change].sum(axis=1)\n",
    "\n",
    "\n",
    "    # Enquiries Sum and Ratio\n",
    "    enq_cols = ['AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "                'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']\n",
    "    existing_enq_cols = [col for col in enq_cols if col in eng_df.columns]\n",
    "    if existing_enq_cols:\n",
    "        eng_df['AMT_ENQ_SUM'] = eng_df[existing_enq_cols].sum(axis=1)\n",
    "        if 'AMT_CREDIT' in eng_df.columns and 'AMT_ENQ_SUM' in eng_df.columns:\n",
    "             eng_df['ENQ_CREDIT_RATIO'] = eng_df['AMT_ENQ_SUM'] / (eng_df['AMT_CREDIT'] + eps)\n",
    "\n",
    "    # Feature: Total missing values count\n",
    "    eng_df['MISSING_VALS_TOTAL_APP'] = eng_df.isnull().sum(axis=1)\n",
    "\n",
    "    print(f\"Numeric feature engineering finished. New shape: {eng_df.shape}\")\n",
    "    return eng_df\n",
    "\n",
    "def engineer_app_categorical_interactions(X_train, X_test, y_train):\n",
    "    \"\"\"Generates features based on categorical groupings and aggregates.\n",
    "       Fits on train, transforms both train and test.\"\"\"\n",
    "    print(\"Engineering categorical interaction features...\")\n",
    "    X_train_eng = X_train.copy()\n",
    "    X_test_eng = X_test.copy()\n",
    "    # Temporarily add target back to X_train for fitting aggregations\n",
    "    X_train_eng['TARGET'] = y_train\n",
    "\n",
    "    # Define groups and aggregations (as per original code)\n",
    "    columns_to_aggregate_on = [\n",
    "        ['NAME_CONTRACT_TYPE', 'NAME_INCOME_TYPE', 'OCCUPATION_TYPE'],\n",
    "        ['CODE_GENDER', 'NAME_FAMILY_STATUS', 'NAME_INCOME_TYPE'],\n",
    "        ['FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE'],\n",
    "        ['NAME_EDUCATION_TYPE', 'NAME_INCOME_TYPE', 'OCCUPATION_TYPE'],\n",
    "        ['OCCUPATION_TYPE', 'ORGANIZATION_TYPE'],\n",
    "        ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']\n",
    "    ]\n",
    "    aggregations = {\n",
    "        'AMT_ANNUITY': ['mean', 'max', 'min'],\n",
    "        'ANNUITY_INCOME_RATIO': ['mean', 'max', 'min'],\n",
    "        'AGE_EMPLOYED_DIFF': ['mean', 'min'],\n",
    "        'AMT_INCOME_TOTAL': ['mean', 'max', 'min'],\n",
    "        # 'APARTMENTS_SUM_AVG': ['mean', 'max', 'min'], # These were created in numeric FE\n",
    "        # 'APARTMENTS_SUM_MEDI': ['mean', 'max', 'min'],# These were created in numeric FE\n",
    "        'EXT_SOURCE_MEAN': ['mean', 'max', 'min'],\n",
    "        'EXT_SOURCE_1': ['mean', 'max', 'min'],\n",
    "        'EXT_SOURCE_2': ['mean', 'max', 'min'],\n",
    "        'EXT_SOURCE_3': ['mean', 'max', 'min'],\n",
    "        # Add target mean for the group (common technique)\n",
    "        'TARGET': ['mean']\n",
    "    }\n",
    "\n",
    "    # Filter groups and aggregations based on available columns\n",
    "    valid_groups = []\n",
    "    for group in columns_to_aggregate_on:\n",
    "        if all(col in X_train_eng.columns for col in group):\n",
    "            valid_groups.append(group)\n",
    "        else:\n",
    "            print(f\"Skipping group due to missing columns: {group}\")\n",
    "\n",
    "    valid_aggregations = {}\n",
    "    for agg_col, agg_funcs in aggregations.items():\n",
    "         if agg_col in X_train_eng.columns:\n",
    "             valid_aggregations[agg_col] = agg_funcs\n",
    "         else:\n",
    "             print(f\"Skipping aggregation on missing column: {agg_col}\")\n",
    "\n",
    "\n",
    "    if not valid_aggregations:\n",
    "         print(\"No valid columns found for categorical interaction aggregations. Skipping.\")\n",
    "         # Drop target added temporarily\n",
    "         X_train_eng.drop(columns=['TARGET'], inplace=True)\n",
    "         return X_train_eng, X_test_eng\n",
    "\n",
    "    # Apply aggregations\n",
    "    for group in valid_groups:\n",
    "        print(f\"Aggregating on group: {group}\")\n",
    "        # Ensure group columns are hashable (e.g. no lists within cells)\n",
    "        try:\n",
    "            grouped_interactions = X_train_eng.groupby(group).agg(valid_aggregations)\n",
    "            new_colnames = ['_'.join(ele).upper() + '_AGG_' + '_'.join(group) for ele in grouped_interactions.columns]\n",
    "            grouped_interactions.columns = new_colnames\n",
    "\n",
    "            # Merge onto train and test\n",
    "            X_train_eng = X_train_eng.merge(grouped_interactions, on=group, how='left')\n",
    "            X_test_eng = X_test_eng.merge(grouped_interactions, on=group, how='left')\n",
    "            print(f\"Merged {len(new_colnames)} features for group {group}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not aggregate on group {group}. Error: {e}\")\n",
    "\n",
    "\n",
    "    # Drop target added temporarily\n",
    "    X_train_eng = X_train_eng.drop(columns=['TARGET'])\n",
    "\n",
    "    print(\"Finished categorical interaction features.\")\n",
    "    return X_train_eng, X_test_eng\n",
    "\n",
    "def encode_app_categorical_response(X_train, X_test, y_train):\n",
    "    \"\"\"Encodes categorical features using Response Encoding.\n",
    "       Fits on train, transforms both train and test.\"\"\"\n",
    "    print(\"Applying Response Encoding...\")\n",
    "    X_train_encoded = X_train.copy()\n",
    "    X_test_encoded = X_test.copy()\n",
    "    X_train_encoded['TARGET'] = y_train # Temporarily add target for fitting\n",
    "\n",
    "    categorical_cols = X_train_encoded.select_dtypes(include='object').columns.tolist()\n",
    "    original_cols_to_drop = []\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        print(f\"Encoding column: {col}\")\n",
    "        # Calculate conditional probabilities P(Category|Target=1) and P(Category|Target=0)\n",
    "        # Smoothed mean: (mean * count + global_mean * alpha) / (count + alpha)\n",
    "        global_mean = X_train_encoded['TARGET'].mean()\n",
    "        agg = X_train_encoded.groupby(col)['TARGET'].agg(['mean', 'count'])\n",
    "        counts = agg['count']\n",
    "        means = agg['mean']\n",
    "        alpha = 10 # Smoothing factor\n",
    "\n",
    "        smoothed_mean = (means * counts + global_mean * alpha) / (counts + alpha)\n",
    "\n",
    "        # Create mapping dictionary (for probability of Target=1)\n",
    "        # For Target=0, it's simply 1 - P(Target=1) conceptually,\n",
    "        # but the original code created two columns based on P(category | label).\n",
    "        # Let's simplify to P(Target=1 | category) which is common target encoding.\n",
    "        # We can add P(Target=0 | category) if needed, it's just 1 minus the first.\n",
    "        mapping_dict_1 = smoothed_mean.to_dict()\n",
    "\n",
    "        # Transform train and test\n",
    "        X_train_encoded[col + '_RESPONSE_1'] = X_train_encoded[col].map(mapping_dict_1)\n",
    "        X_test_encoded[col + '_RESPONSE_1'] = X_test_encoded[col].map(mapping_dict_1)\n",
    "\n",
    "        # Fill potential NaNs in test set (categories not seen in train) with global mean\n",
    "        X_test_encoded[col + '_RESPONSE_1'] = X_test_encoded[col + '_RESPONSE_1'].fillna(global_mean)\n",
    "\n",
    "        # Add P(Target=0 | category) column\n",
    "        # X_train_encoded[col + '_RESPONSE_0'] = 1 - X_train_encoded[col + '_RESPONSE_1']\n",
    "        # X_test_encoded[col + '_RESPONSE_0'] = 1 - X_test_encoded[col + '_RESPONSE_1']\n",
    "\n",
    "        original_cols_to_drop.append(col)\n",
    "\n",
    "    # Drop original categorical columns and the temporary target column\n",
    "    X_train_encoded = X_train_encoded.drop(columns=original_cols_to_drop + ['TARGET'])\n",
    "    X_test_encoded = X_test_encoded.drop(columns=original_cols_to_drop)\n",
    "\n",
    "    print(f\"Response Encoding finished. Dropped original columns: {original_cols_to_drop}\")\n",
    "    return X_train_encoded, X_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Bureau & Bureau Balance Preprocessing Functions ---\n",
    "\n",
    "def preprocess_bureau_balance(bureau_balance_df):\n",
    "    \"\"\"Preprocesses bureau_balance: encodes status, creates weighted/EWM features, aggregates by SK_ID_BUREAU.\"\"\"\n",
    "    print(\"Preprocessing bureau_balance...\")\n",
    "    bb = bureau_balance_df.copy()\n",
    "    initial_shape = bb.shape\n",
    "\n",
    "    # Encode STATUS\n",
    "    # Original mapping: {'C': 0, '0': 1, '1': 2, '2': 3, 'X': 4, '3': 5, '4': 6, '5': 7}\n",
    "    # Simpler numeric mapping might also work, but let's keep the original logic\n",
    "    dict_for_status = {'C': 0, '0': 1, '1': 2, '2': 3, 'X': 4, '3': 5, '4': 6, '5': 7}\n",
    "    bb['STATUS_ENCODED'] = bb['STATUS'].map(dict_for_status)\n",
    "\n",
    "    # Weighted status\n",
    "    bb['MONTHS_BALANCE_POS'] = np.abs(bb['MONTHS_BALANCE'])\n",
    "    bb['WEIGHTED_STATUS'] = bb['STATUS_ENCODED'] / (bb['MONTHS_BALANCE_POS'] + 1)\n",
    "\n",
    "    # Sort for EWM calculation\n",
    "    bb = bb.sort_values(by=['SK_ID_BUREAU', 'MONTHS_BALANCE_POS'], ascending=[True, True]) # Ascending months\n",
    "\n",
    "    # Exponential Weighted Moving Average (EWMA)\n",
    "    alpha = 0.8\n",
    "    bb['EXP_WEIGHTED_STATUS'] = bb.groupby('SK_ID_BUREAU')['WEIGHTED_STATUS'].transform(lambda x: x.ewm(alpha=alpha).mean())\n",
    "    bb['EXP_ENCODED_STATUS'] = bb.groupby('SK_ID_BUREAU')['STATUS_ENCODED'].transform(lambda x: x.ewm(alpha=alpha).mean())\n",
    "\n",
    "    # Yearly Aggregations\n",
    "    bb['YEAR_BALANCE'] = bb['MONTHS_BALANCE_POS'] // 12\n",
    "\n",
    "    # Define aggregations\n",
    "    aggregations_basic = {\n",
    "        'MONTHS_BALANCE_POS': ['count', 'mean', 'max'], # Added count\n",
    "        'STATUS_ENCODED': ['mean', 'max', 'first'],\n",
    "        'WEIGHTED_STATUS': ['mean', 'sum', 'first'],\n",
    "        'EXP_ENCODED_STATUS': ['last'],\n",
    "        'EXP_WEIGHTED_STATUS': ['last']\n",
    "    }\n",
    "    aggregations_for_year = {\n",
    "        'STATUS_ENCODED': ['mean', 'max', 'last', 'first'],\n",
    "        'WEIGHTED_STATUS': ['mean', 'max', 'first', 'last'],\n",
    "        'EXP_WEIGHTED_STATUS': ['last'],\n",
    "        'EXP_ENCODED_STATUS': ['last']\n",
    "    }\n",
    "\n",
    "    # Aggregate overall\n",
    "    aggregated_bb = bb.groupby('SK_ID_BUREAU').agg(aggregations_basic)\n",
    "    aggregated_bb.columns = ['BB_' + '_'.join(col).upper() for col in aggregated_bb.columns]\n",
    "\n",
    "    # Aggregate yearly (last 2 years + rest)\n",
    "    aggregated_bb_years = pd.DataFrame(index=aggregated_bb.index) # Start with same index\n",
    "    for year in range(2):\n",
    "        year_group = bb[bb['YEAR_BALANCE'] == year].groupby('SK_ID_BUREAU').agg(aggregations_for_year)\n",
    "        year_group.columns = [f'BB_{\"_\".join(col).upper()}_YEAR_{year}' for col in year_group.columns]\n",
    "        aggregated_bb_years = aggregated_bb_years.merge(year_group, on='SK_ID_BUREAU', how='left')\n",
    "\n",
    "    aggregated_bb_rest_years = bb[bb['YEAR_BALANCE'] >= 2].groupby('SK_ID_BUREAU').agg(aggregations_for_year)\n",
    "    aggregated_bb_rest_years.columns = [f'BB_{\"_\".join(col).upper()}_YEAR_REST' for col in aggregated_bb_rest_years.columns]\n",
    "    aggregated_bb_years = aggregated_bb_years.merge(aggregated_bb_rest_years, on='SK_ID_BUREAU', how='left')\n",
    "\n",
    "    # Merge yearly aggregates back\n",
    "    aggregated_bb = aggregated_bb.merge(aggregated_bb_years, on='SK_ID_BUREAU', how='left')\n",
    "\n",
    "    # Fill NaNs resulting from aggregations (e.g., bureaus with no recent history)\n",
    "    # Filling with 0 might be okay, but consider if mean/median is better for some fields\n",
    "    aggregated_bb = aggregated_bb.fillna(0)\n",
    "\n",
    "    print(f\"Finished preprocessing bureau_balance. Initial shape: {initial_shape}, Final aggregated shape: {aggregated_bb.shape}\")\n",
    "    return aggregated_bb\n",
    "\n",
    "def preprocess_bureau(bureau_df, aggregated_bb):\n",
    "    \"\"\"Preprocesses bureau: merges with aggregated bureau_balance, cleans, engineers features,\n",
    "       aggregates by SK_ID_CURR.\"\"\"\n",
    "    print(\"Preprocessing bureau data...\")\n",
    "    bureau = bureau_df.copy()\n",
    "    initial_shape = bureau.shape\n",
    "\n",
    "    # Merge with aggregated bureau_balance\n",
    "    bureau_merged = bureau.merge(aggregated_bb, on='SK_ID_BUREAU', how='left')\n",
    "    print(f\"Merged bureau with aggregated bureau_balance. Shape: {bureau_merged.shape}\")\n",
    "\n",
    "    # Clean DAYS fields (limiting to past 50 years ~ 18250 days)\n",
    "    # Use .loc for assignment to avoid warnings\n",
    "    limit_days = -50 * 365\n",
    "    cols_to_clean = ['DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'DAYS_CREDIT_UPDATE']\n",
    "    for col in cols_to_clean:\n",
    "        if col in bureau_merged.columns:\n",
    "             bureau_merged.loc[bureau_merged[col] < limit_days, col] = np.nan\n",
    "\n",
    "    # Feature Engineering\n",
    "    eps = 1e-5\n",
    "    if 'DAYS_CREDIT' in bureau_merged.columns and 'DAYS_CREDIT_ENDDATE' in bureau_merged.columns:\n",
    "         bureau_merged['B_CREDIT_DURATION'] = bureau_merged['DAYS_CREDIT_ENDDATE'] - bureau_merged['DAYS_CREDIT'] # End - Start\n",
    "    if 'CREDIT_DAY_OVERDUE' in bureau_merged.columns:\n",
    "         bureau_merged['B_FLAG_OVERDUE_RECENT'] = (bureau_merged['CREDIT_DAY_OVERDUE'] > 0).astype(int)\n",
    "    if 'AMT_CREDIT_MAX_OVERDUE' in bureau_merged.columns and 'B_CREDIT_DURATION' in bureau_merged.columns:\n",
    "         bureau_merged['B_MAX_AMT_OVERDUE_DURATION_RATIO'] = bureau_merged['AMT_CREDIT_MAX_OVERDUE'] / (bureau_merged['B_CREDIT_DURATION'] + eps)\n",
    "    if 'AMT_CREDIT_SUM_OVERDUE' in bureau_merged.columns and 'B_CREDIT_DURATION' in bureau_merged.columns:\n",
    "         bureau_merged['B_CURRENT_AMT_OVERDUE_DURATION_RATIO'] = bureau_merged['AMT_CREDIT_SUM_OVERDUE'] / (bureau_merged['B_CREDIT_DURATION'] + eps)\n",
    "    if 'AMT_CREDIT_SUM_OVERDUE' in bureau_merged.columns and 'DAYS_CREDIT_ENDDATE' in bureau_merged.columns:\n",
    "         # Need positive days for ratio denominator - use abs or adjust logic if needed\n",
    "         bureau_merged['B_AMT_OVERDUE_DURATION_LEFT_RATIO'] = bureau_merged['AMT_CREDIT_SUM_OVERDUE'] / (np.abs(bureau_merged['DAYS_CREDIT_ENDDATE']) + eps)\n",
    "    if 'CNT_CREDIT_PROLONG' in bureau_merged.columns and 'AMT_CREDIT_MAX_OVERDUE' in bureau_merged.columns:\n",
    "         bureau_merged['B_CNT_PROLONGED_MAX_OVERDUE_MUL'] = bureau_merged['CNT_CREDIT_PROLONG'] * bureau_merged['AMT_CREDIT_MAX_OVERDUE']\n",
    "    if 'CNT_CREDIT_PROLONG' in bureau_merged.columns and 'B_CREDIT_DURATION' in bureau_merged.columns:\n",
    "         bureau_merged['B_CNT_PROLONGED_DURATION_RATIO'] = bureau_merged['CNT_CREDIT_PROLONG'] / (bureau_merged['B_CREDIT_DURATION'] + eps)\n",
    "    if 'AMT_CREDIT_SUM_DEBT' in bureau_merged.columns and 'AMT_CREDIT_SUM' in bureau_merged.columns:\n",
    "         bureau_merged['B_CURRENT_DEBT_TO_CREDIT_RATIO'] = bureau_merged['AMT_CREDIT_SUM_DEBT'] / (bureau_merged['AMT_CREDIT_SUM'] + eps)\n",
    "         bureau_merged['B_CURRENT_CREDIT_DEBT_DIFF'] = bureau_merged['AMT_CREDIT_SUM'] - bureau_merged['AMT_CREDIT_SUM_DEBT']\n",
    "    if 'AMT_ANNUITY' in bureau_merged.columns and 'AMT_CREDIT_SUM' in bureau_merged.columns:\n",
    "         bureau_merged['B_AMT_ANNUITY_CREDIT_RATIO'] = bureau_merged['AMT_ANNUITY'] / (bureau_merged['AMT_CREDIT_SUM'] + eps)\n",
    "    if 'DAYS_CREDIT_UPDATE' in bureau_merged.columns and 'DAYS_CREDIT_ENDDATE' in bureau_merged.columns:\n",
    "        # Difference in days (both should be negative, difference magnitude matters)\n",
    "        bureau_merged['B_CREDIT_ENDDATE_UPDATE_DIFF'] = bureau_merged['DAYS_CREDIT_ENDDATE'] - bureau_merged['DAYS_CREDIT_UPDATE']\n",
    "\n",
    "    # Aggregation by SK_ID_CURR\n",
    "    print(\"Aggregating bureau features by SK_ID_CURR...\")\n",
    "\n",
    "    # --- Aggregation Strategy ---\n",
    "    # 1. Aggregate numeric features overall (mean, sum, max, min, std)\n",
    "    # 2. One-hot encode categoricals, then aggregate the OHE features (mean gives prevalence)\n",
    "    # 3. Aggregate features grouped by CREDIT_ACTIVE status (as per original code)\n",
    "\n",
    "    # 1. Overall Numeric Aggregations\n",
    "    numeric_cols_bureau = bureau_merged.select_dtypes(include=np.number).columns.tolist()\n",
    "    # Remove IDs from aggregation list\n",
    "    numeric_cols_bureau = [col for col in numeric_cols_bureau if col not in ['SK_ID_CURR', 'SK_ID_BUREAU']]\n",
    "\n",
    "    aggregations_numeric = {}\n",
    "    for col in numeric_cols_bureau:\n",
    "         aggregations_numeric[col] = ['mean', 'max', 'min', 'sum', 'std']\n",
    "\n",
    "    bureau_agg_numeric = bureau_merged.groupby('SK_ID_CURR').agg(aggregations_numeric)\n",
    "    bureau_agg_numeric.columns = ['B_' + '_'.join(col).upper() + '_OVERALL' for col in bureau_agg_numeric.columns]\n",
    "    bureau_agg_numeric = bureau_agg_numeric.fillna(0) # Fill NaNs from aggregation (e.g., std dev for single entry)\n",
    "\n",
    "    # 2. Categorical Aggregations (One-Hot Encoding then Mean)\n",
    "    categorical_cols_bureau = bureau_merged.select_dtypes(include='object').columns.tolist()\n",
    "    bureau_for_ohe = bureau_merged[['SK_ID_CURR'] + categorical_cols_bureau].copy()\n",
    "    bureau_for_ohe = bureau_for_ohe.fillna('MISSING') # Fill NaNs before OHE\n",
    "\n",
    "    # Using pandas get_dummies for simplicity here\n",
    "    bureau_ohe = pd.get_dummies(bureau_for_ohe, columns=categorical_cols_bureau, prefix_sep='_')\n",
    "\n",
    "    # Aggregate OHE features by mean\n",
    "    bureau_agg_categorical = bureau_ohe.groupby('SK_ID_CURR').mean()\n",
    "    bureau_agg_categorical.columns = ['B_' + col.upper() + '_MEAN' for col in bureau_agg_categorical.columns] # Prefix columns\n",
    "\n",
    "    # 3. Aggregations by CREDIT_ACTIVE status\n",
    "    # Define aggregations for status groups\n",
    "    aggregations_credit_active = {\n",
    "        'DAYS_CREDIT': ['mean', 'min', 'max'],\n",
    "        'CREDIT_DAY_OVERDUE': ['mean', 'max', 'sum'], # Added sum\n",
    "        'DAYS_CREDIT_ENDDATE': ['mean', 'max', 'min'], # Added min\n",
    "        'AMT_CREDIT_SUM': ['sum', 'mean', 'max'], # Added mean\n",
    "        'AMT_CREDIT_SUM_DEBT': ['sum', 'mean'], # Added mean\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['max', 'sum', 'mean'], # Added mean\n",
    "        'AMT_ANNUITY': ['mean', 'sum', 'max'],\n",
    "        'B_CREDIT_DURATION': ['max', 'mean', 'min', 'std'], # Added min, std\n",
    "        # Add aggregates from bureau_balance perspective\n",
    "        'BB_STATUS_ENCODED_MEAN': ['mean', 'max'],\n",
    "        'BB_WEIGHTED_STATUS_MEAN': ['mean', 'max'],\n",
    "        'BB_MONTHS_BALANCE_POS_COUNT': ['sum', 'mean'] # How many total months reported per status\n",
    "    }\n",
    "    # Filter based on available columns in bureau_merged\n",
    "    valid_aggs_credit_active = {}\n",
    "    for col, funcs in aggregations_credit_active.items():\n",
    "         if col in bureau_merged.columns:\n",
    "             valid_aggs_credit_active[col] = funcs\n",
    "\n",
    "    bureau_agg_by_status = pd.DataFrame(index=bureau_agg_numeric.index) # Start with all SK_ID_CURR\n",
    "    active_statuses = bureau_merged['CREDIT_ACTIVE'].unique()\n",
    "\n",
    "    for status in active_statuses:\n",
    "        if pd.isna(status): continue # Skip potential NaN group if exists\n",
    "        status_safe_name = status.replace(' ', '_').replace('/', '_') # Make status name filename-safe\n",
    "        group = bureau_merged[bureau_merged['CREDIT_ACTIVE'] == status].groupby('SK_ID_CURR').agg(valid_aggs_credit_active)\n",
    "        group.columns = [f'B_{\"_\".join(col).upper()}_CA_{status_safe_name.upper()}' for col in group.columns]\n",
    "        bureau_agg_by_status = bureau_agg_by_status.merge(group, on='SK_ID_CURR', how='left')\n",
    "\n",
    "    bureau_agg_by_status = bureau_agg_by_status.fillna(0) # Fill NaNs for customers without certain credit statuses\n",
    "\n",
    "    # Combine all aggregated parts\n",
    "    final_bureau_agg = bureau_agg_numeric.merge(bureau_agg_categorical, on='SK_ID_CURR', how='left')\n",
    "    final_bureau_agg = final_bureau_agg.merge(bureau_agg_by_status, on='SK_ID_CURR', how='left')\n",
    "\n",
    "    print(f\"Finished aggregating bureau data. Initial shape: {initial_shape}, Final aggregated shape: {final_bureau_agg.shape}\")\n",
    "    return final_bureau_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_final_data(X_train, X_test, y_train, y_test, bureau_aggregated):\n",
    "    \"\"\"Merges the processed application splits with aggregated bureau data.\"\"\"\n",
    "    print(\"Merging aggregated bureau data with application train/test sets...\")\n",
    "\n",
    "    # Make sure X_train and y_train have the same index before combining\n",
    "    assert len(X_train) == len(y_train), \"X_train and y_train must have the same length\"\n",
    "    assert len(X_test) == len(y_test), \"X_test and y_test must have the same length\"\n",
    "    \n",
    "    # Make copies to avoid modifying originals\n",
    "    train_df = X_train.copy()\n",
    "    test_df = X_test.copy()\n",
    "    \n",
    "    # Ensure indices match before assigning TARGET\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    y_train_reset = y_train.reset_index(drop=True)\n",
    "    train_df['TARGET'] = y_train_reset\n",
    "    \n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    y_test_reset = y_test.reset_index(drop=True)\n",
    "    test_df['TARGET'] = y_test_reset\n",
    "    \n",
    "    # Ensure SK_ID_CURR is available as a column for merging\n",
    "    if 'SK_ID_CURR' not in train_df.columns:\n",
    "        print(\"Error: SK_ID_CURR not in dataframe columns. Cannot proceed with merge.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Check for duplicate SK_ID_CURR values before merging\n",
    "    train_duplicates = train_df['SK_ID_CURR'].duplicated().sum()\n",
    "    if train_duplicates > 0:\n",
    "        print(f\"Warning: Found {train_duplicates} duplicate SK_ID_CURR values in train_df\")\n",
    "    \n",
    "    bureau_duplicates = bureau_aggregated['SK_ID_CURR'].duplicated().sum() if 'SK_ID_CURR' in bureau_aggregated.columns else 0\n",
    "    if bureau_duplicates > 0:\n",
    "        print(f\"Warning: Found {bureau_duplicates} duplicate SK_ID_CURR values in bureau_aggregated\")\n",
    "    \n",
    "    # Merge using SK_ID_CURR\n",
    "    train_merged = train_df.merge(bureau_aggregated.reset_index() if bureau_aggregated.index.name == 'SK_ID_CURR' \n",
    "                                  else bureau_aggregated, \n",
    "                                  on='SK_ID_CURR', how='left')\n",
    "    test_merged = test_df.merge(bureau_aggregated.reset_index() if bureau_aggregated.index.name == 'SK_ID_CURR' \n",
    "                                else bureau_aggregated, \n",
    "                                on='SK_ID_CURR', how='left')\n",
    "\n",
    "    # Verify no TARGET NaNs were introduced during merge\n",
    "    train_na_count = train_merged['TARGET'].isna().sum()\n",
    "    if train_na_count > 0:\n",
    "        print(f\"Warning: {train_na_count} NaN values found in TARGET after merge\")\n",
    "    \n",
    "    # Fill NaNs in bureau columns\n",
    "    bureau_cols = [col for col in bureau_aggregated.columns if col != 'SK_ID_CURR']\n",
    "    train_merged[bureau_cols] = train_merged[bureau_cols].fillna(0)\n",
    "    test_merged[bureau_cols] = test_merged[bureau_cols].fillna(0)\n",
    "\n",
    "    print(f\"Merging finished. Final Train shape: {train_merged.shape}, Final Test shape: {test_merged.shape}\")\n",
    "    return train_merged, test_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Main Orchestrator Function ---\n",
    "\n",
    "def preprocess_credit_data(data_path='../data/raw/', test_size=0.2, random_state=42):\n",
    "    \"\"\"Orchestrates the loading, preprocessing, feature engineering, and merging.\"\"\"\n",
    "    start_time = datetime.now()\n",
    "    print(f\"Starting Credit Data Preprocessing at: {start_time}\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    app_train_df, bureau_df, bureau_balance_df = load_initial_data(data_path)\n",
    "\n",
    "    # 1a. Pre-Split Cleaning (if any absolutely required before split)\n",
    "    # Handle 'XNA' Gender before splitting, as it removes rows\n",
    "    if 'CODE_GENDER' in app_train_df.columns:\n",
    "        initial_rows = app_train_df.shape[0]\n",
    "        app_train_df = app_train_df[app_train_df['CODE_GENDER'] != 'XNA'].copy()\n",
    "        rows_removed = initial_rows - app_train_df.shape[0]\n",
    "        if rows_removed > 0:\n",
    "            print(f\"Removed {rows_removed} rows with 'XNA' gender before splitting.\")\n",
    "\n",
    "    # 2. Split Application Data\n",
    "    X_train, X_test, y_train, y_test = split_application_data(app_train_df, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # 3. Preprocess Application Data (Post-Split)\n",
    "    X_train = clean_application(X_train)\n",
    "    X_test = clean_application(X_test)\n",
    "\n",
    "    X_train, X_test = handle_app_missing_categorical(X_train, X_test) # Fill categorical NaNs\n",
    "\n",
    "    # Impute EXT_SOURCE - Fit on Train, Transform Train & Test\n",
    "    X_train, X_test = impute_ext_sources_xgb(X_train, X_test)\n",
    "\n",
    "    # Numeric Feature Engineering - Apply to Train & Test\n",
    "    X_train = engineer_app_numeric_features(X_train)\n",
    "    X_test = engineer_app_numeric_features(X_test)\n",
    "\n",
    "    # Categorical Interactions - Fit on Train, Transform Train & Test\n",
    "    X_train, X_test = engineer_app_categorical_interactions(X_train, X_test, y_train)\n",
    "\n",
    "    # Categorical Encoding (Response Encoding) - Fit on Train, Transform Train & Test\n",
    "    X_train, X_test = encode_app_categorical_response(X_train, X_test, y_train)\n",
    "\n",
    "    # 4. Preprocess Bureau & Bureau Balance\n",
    "    aggregated_bb = preprocess_bureau_balance(bureau_balance_df)\n",
    "    aggregated_bureau = preprocess_bureau(bureau_df, aggregated_bb)\n",
    "\n",
    "    # 5. Merge Application and Bureau Data\n",
    "    train_final, test_final = merge_final_data(X_train, X_test, y_train, y_test, aggregated_bureau)\n",
    "\n",
    "    # 6. Final Checks\n",
    "    print(\"\\nFinal NaN check (Train):\")\n",
    "    print(train_final.isnull().sum().sort_values(ascending=False).head(10))\n",
    "    print(\"\\nFinal NaN check (Test):\")\n",
    "    print(test_final.isnull().sum().sort_values(ascending=False).head(10))\n",
    "    \n",
    "    # Fill remaining NaNs with 0 (if any)\n",
    "    train_final = train_final.fillna(0)\n",
    "    test_final = test_final.fillna(0)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print(f\"\\nPreprocessing finished at: {end_time}\")\n",
    "    print(f\"Total time taken: {end_time - start_time}\")\n",
    "\n",
    "    return train_final, test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Credit Data Preprocessing at: 2025-04-05 15:47:34.187197\n",
      "Loading raw data...\n",
      "Raw data loaded successfully.\n",
      "Removed 4 rows with 'XNA' gender before splitting.\n",
      "Splitting application data (Test size: 0.2, Random State: 42)...\n",
      "Train shape: (246005, 121), Test shape: (61502, 121)\n",
      "Applying basic cleaning to DataFrame with shape: (246005, 121)\n",
      "Dropped low-variance FLAG columns: ['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_20']\n",
      "Converted DAYS_BIRTH to years.\n",
      "Replaced DAYS_EMPLOYED anomaly (365243) with NaN.\n",
      "Capped OBS_30_CNT_SOCIAL_CIRCLE at 30 (values > 30 set to NaN).\n",
      "Capped OBS_60_CNT_SOCIAL_CIRCLE at 30 (values > 30 set to NaN).\n",
      "Converted REGION_RATING_CLIENT to object type.\n",
      "Converted REGION_RATING_CLIENT_W_CITY to object type.\n",
      "Basic cleaning finished.\n",
      "Applying basic cleaning to DataFrame with shape: (61502, 121)\n",
      "Dropped low-variance FLAG columns: ['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_20']\n",
      "Converted DAYS_BIRTH to years.\n",
      "Replaced DAYS_EMPLOYED anomaly (365243) with NaN.\n",
      "Capped OBS_30_CNT_SOCIAL_CIRCLE at 30 (values > 30 set to NaN).\n",
      "Capped OBS_60_CNT_SOCIAL_CIRCLE at 30 (values > 30 set to NaN).\n",
      "Converted REGION_RATING_CLIENT to object type.\n",
      "Converted REGION_RATING_CLIENT_W_CITY to object type.\n",
      "Basic cleaning finished.\n",
      "Handling missing categorical values (filling with 'XNA')...\n",
      "Filled NaNs in 18 categorical columns.\n",
      "Imputing EXT_SOURCE columns using XGBoost...\n",
      "Imputing EXT_SOURCE_2...\n",
      "Trained XGBoost model for EXT_SOURCE_2.\n",
      "Imputed 523 values in train set for EXT_SOURCE_2.\n",
      "Imputed 137 values in test set for EXT_SOURCE_2.\n",
      "Imputing EXT_SOURCE_3...\n",
      "Trained XGBoost model for EXT_SOURCE_3.\n",
      "Imputed 48727 values in train set for EXT_SOURCE_3.\n",
      "Imputed 12238 values in test set for EXT_SOURCE_3.\n",
      "Imputing EXT_SOURCE_1...\n",
      "Trained XGBoost model for EXT_SOURCE_1.\n",
      "Imputed 138773 values in train set for EXT_SOURCE_1.\n",
      "Imputed 34603 values in test set for EXT_SOURCE_1.\n",
      "Finished EXT_SOURCE imputation.\n",
      "Applying numeric feature engineering to DataFrame with shape: (246005, 116)\n",
      "Numeric feature engineering finished. New shape: (246005, 167)\n",
      "Applying numeric feature engineering to DataFrame with shape: (61502, 116)\n",
      "Numeric feature engineering finished. New shape: (61502, 167)\n",
      "Engineering categorical interaction features...\n",
      "Aggregating on group: ['NAME_CONTRACT_TYPE', 'NAME_INCOME_TYPE', 'OCCUPATION_TYPE']\n",
      "Merged 24 features for group ['NAME_CONTRACT_TYPE', 'NAME_INCOME_TYPE', 'OCCUPATION_TYPE'].\n",
      "Aggregating on group: ['CODE_GENDER', 'NAME_FAMILY_STATUS', 'NAME_INCOME_TYPE']\n",
      "Merged 24 features for group ['CODE_GENDER', 'NAME_FAMILY_STATUS', 'NAME_INCOME_TYPE'].\n",
      "Aggregating on group: ['FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE']\n",
      "Merged 24 features for group ['FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE'].\n",
      "Aggregating on group: ['NAME_EDUCATION_TYPE', 'NAME_INCOME_TYPE', 'OCCUPATION_TYPE']\n",
      "Merged 24 features for group ['NAME_EDUCATION_TYPE', 'NAME_INCOME_TYPE', 'OCCUPATION_TYPE'].\n",
      "Aggregating on group: ['OCCUPATION_TYPE', 'ORGANIZATION_TYPE']\n",
      "Merged 24 features for group ['OCCUPATION_TYPE', 'ORGANIZATION_TYPE'].\n",
      "Aggregating on group: ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']\n",
      "Merged 24 features for group ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY'].\n",
      "Finished categorical interaction features.\n",
      "Applying Response Encoding...\n",
      "Encoding column: NAME_CONTRACT_TYPE\n",
      "Encoding column: CODE_GENDER\n",
      "Encoding column: FLAG_OWN_CAR\n",
      "Encoding column: FLAG_OWN_REALTY\n",
      "Encoding column: NAME_TYPE_SUITE\n",
      "Encoding column: NAME_INCOME_TYPE\n",
      "Encoding column: NAME_EDUCATION_TYPE\n",
      "Encoding column: NAME_FAMILY_STATUS\n",
      "Encoding column: NAME_HOUSING_TYPE\n",
      "Encoding column: OCCUPATION_TYPE\n",
      "Encoding column: WEEKDAY_APPR_PROCESS_START\n",
      "Encoding column: ORGANIZATION_TYPE\n",
      "Encoding column: FONDKAPREMONT_MODE\n",
      "Encoding column: HOUSETYPE_MODE\n",
      "Encoding column: WALLSMATERIAL_MODE\n",
      "Encoding column: EMERGENCYSTATE_MODE\n",
      "Response Encoding finished. Dropped original columns: ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE']\n",
      "Preprocessing bureau_balance...\n",
      "Finished preprocessing bureau_balance. Initial shape: (14701612, 3), Final aggregated shape: (523515, 41)\n",
      "Preprocessing bureau data...\n",
      "Merged bureau with aggregated bureau_balance. Shape: (1465325, 58)\n",
      "Aggregating bureau features by SK_ID_CURR...\n",
      "Finished aggregating bureau data. Initial shape: (1465325, 17), Final aggregated shape: (263491, 463)\n",
      "Merging aggregated bureau data with application train/test sets...\n",
      "Merging finished. Final Train shape: (246005, 775), Final Test shape: (61502, 775)\n",
      "\n",
      "Final NaN check (Train):\n",
      "COMMONAREA_MEDI             171920\n",
      "COMMONAREA_MODE             171920\n",
      "COMMONAREA_AVG              171920\n",
      "NONLIVINGAPARTMENTS_AVG     170799\n",
      "NONLIVINGAPARTMENTS_MEDI    170799\n",
      "NONLIVINGAPARTMENTS_MODE    170799\n",
      "CAR_EMPLOYED_DIFF           170383\n",
      "CAR_EMPLOYED_RATIO          170383\n",
      "LIVINGAPARTMENTS_AVG        168135\n",
      "LIVINGAPARTMENTS_MEDI       168135\n",
      "dtype: int64\n",
      "\n",
      "Final NaN check (Test):\n",
      "COMMONAREA_MEDI             42942\n",
      "COMMONAREA_AVG              42942\n",
      "COMMONAREA_MODE             42942\n",
      "NONLIVINGAPARTMENTS_AVG     42713\n",
      "NONLIVINGAPARTMENTS_MODE    42713\n",
      "CAR_EMPLOYED_DIFF           42713\n",
      "CAR_EMPLOYED_RATIO          42713\n",
      "NONLIVINGAPARTMENTS_MEDI    42713\n",
      "LIVINGAPARTMENTS_MEDI       42062\n",
      "LIVINGAPARTMENTS_AVG        42062\n",
      "dtype: int64\n",
      "\n",
      "Preprocessing finished at: 2025-04-05 15:49:31.512261\n",
      "Total time taken: 0:01:57.325064\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_DIRECTORY = '../data/raw/' # Adjust path as needed\n",
    "    try:\n",
    "        final_train_data, final_test_data = preprocess_credit_data(data_path=DATA_DIRECTORY)\n",
    "        final_train_data.to_csv('../data/processed/train_final.csv', index=False)\n",
    "        final_test_data.to_csv('../data/processed/test_final.csv', index=False)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nExecution failed due to missing files. Please check the DATA_DIRECTORY path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during preprocessing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
